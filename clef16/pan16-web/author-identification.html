<!DOCTYPE html>
<html lang="en">
<head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>PAN 2016</title> 

<link href="../../css/bootstrap.min.css" rel="stylesheet" />
<link href="../../css/prettify.css" rel="stylesheet" />

<style>
.navbar .navbar-nav {
  font-weight: bold;
}
</style>

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
  <script src="../../js/html5shiv.js"></script>
  <script src="../../js/respond.min.js"></script>
<![endif]-->

<link rel="shortcut icon" href="../pan16-figures/pan-icon-16x16.ico">
<!--
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
<link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
-->

</head>
<body>

<nav class="navbar navbar-inverse navbar-static-top" role="navigation">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="../../index.html"><img src="../pan16-figures/pan-logo-small-lightgrey.png" alt="PAN" style="margin-top:-5px"></a>
  </div>
  <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
    <ul class="nav navbar-nav navbar-right">
      <li><a href="../../index.html">Home</a></li>
      <li><a href="index.html">PAN @ CLEF 2016:</a></li>
      <li><a href="about.html">About</a></li>
      <li><a href="proceedings.html">Proceedings</a></li>
      <li class="dropdown active">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Tasks <span class="caret"></span></a>
        <ul class="dropdown-menu">
          <li class="active"><a href="author-identification.html">Author Identification</a></li>
          <li><a href="author-profiling.html">Author Profiling</a></li>
          <li><a href="author-obfuscation.html">Author Obfuscation</a></li>
        </ul>
      </li>
    </ul>
  </div>
</nav>

<div class="container">

<div class="row">
  <div class="col-xs-12">
    <h1 id="task-description" class="page-header">
      Author Identification
      <div class="btn-group">
        <a role="button" class="btn btn-default" href="../../clef15/pan15-web/author-identification.html"><span class="glyphicon glyphicon-chevron-left" style="color:grey;"></span></a>
        <span role="button" class="btn btn-default" disabled="disabled">2016</span>
        <span role="button" class="btn btn-default" disabled="disabled"><span class="glyphicon glyphicon-chevron-right" style="color:grey;"></span></span>
      </div>
    </h1>
    <div class="thumbnail pull-right" style="text-align:right;margin-left:15px;"><a href="http://www.adobe.com/" target="_blank"><img src="../pan16-figures/logo-adobe.png" alt="Adobe" style="max-height:150px"></a><div style="font-size:7pt;margin-right:13px;margin-top:2px;">Sponsor</div></div>
    <p class="lead">This task is divided into <strong>author clustering</strong> and <strong>author diarization</strong>. You can choose to solve one or both of them.</p>
    <ul class="nav nav-pills visible-xs">
      <li><a href="#author-clustering" class="btn btn-large"><span class="glyphicon glyphicon-chevron-down"></span> Author Clustering</a></li>
      <li><a href="#author-diarization" class="btn btn-large"><span class="glyphicon glyphicon-chevron-down"></span> Author Diarization</a></li>
    </ul>
  </div>
</div>

<div class="row">
  <div class="col-sm-6">
    <h2 id="author-clustering">Author Clustering</h2>
    <p>Authorship attribution is an important problem in information retrieval and computational linguistics but also in applied areas such as law and journalism where knowing the author of a document (such as a ransom note) may be able to save lives. The most common framework for testing candidate algorithms is the closed-set attribution task: given known sample documents from a small, finite set of candidate authors, which wrote a questioned document of unknown authorship? It has been commented, however, that this may be an unreasonably easy task. A more demanding task is <strong>author clustering</strong> where given a document collection the task is to group documents written by the same author so that each cluster corresponds to a different author. This task can also be viewed as establishing <strong>authorship links</strong> between documents. </p>
	<p>Note that a variation of author clustering was included in the PAN-2012 edition. However, it was focused on the paragraph-level and therefore it is more related to the author diarization task (see details in the right column). In PAN-2016, we focus on <strong>document-level author clustering</strong>. The task of authorship verification studied in detail in previous editions of PAN (2013-2015) is strongly associated with author clustering since any clustering problem can be decomposed into a series of author verification problems. We encourage participants to attempt to modify authorship verification approaches to deal with the author clustering task.</p>
	<p>In this edition of PAN we aim to study two application scenarios: </p>
	<ul>
	<li><strong>Complete author clustering</strong>: This scenario requires a detailed analysis where the number (k) of different authors found in the collection should be identified and each document should be assigned to exactly one of the k clusters (each cluster corresponds to a different author). In the following illustrating example, 4 different authors are found and the colour of each document indicates its author.</li>
		<p align="center"><img src="https://cloud.githubusercontent.com/assets/15824066/12065874/81d95440-afe7-11e5-828b-54e293540823.png"/></p>
	<li><strong>Authorship-link ranking</strong>: This scenario views the exploration of the given document collection as a retrieval task. It aims at establishing authorship links between documents and provides a list of document pairs ranked according to a confidence score (the score shows how likely it is the document pair to be by the same author). In the following example, 4 document pairs with similar authorship are found and then these authorship-links are sorted according to their similarity.</li>
		<p align="center"><img src="https://cloud.githubusercontent.com/assets/15824066/12065971/f2a087e2-afe8-11e5-9ef4-6df4e5aff9ae.png"/></p>
	</ul>
    <div class="panel panel-default">
      <div class="panel-heading">Task</div>
      <div class="panel-body">Given a collection of (up to 100) documents, identify authorship links and groups of documents by the same author. All documents are single-authored, in the same language, and belong to the same genre. However, the topic or text-length of documents may vary. The number of distinct authors whose documents are included in the collection is not given.</div>
    </div>
    <div class="panel panel-default">
      <div class="panel-heading">Training Phase</div>
      <div class="panel-body"><p>To develop your software, we provide you with a training corpus that comprises a set of author clustering problems in <strong>3 languages</strong> (English, Dutch, and Greek) and <strong>2 genres</strong> (newspaper articles and reviews). Each problem consists of a set of documents in the same language and genre. However, their topic may differ and the document lengths vary from a few hundred to a few thousand words.</p>
	  <p>The documents of each problem are located in a separate folder. The file <code>info.json</code> describes all required information for each clustering problem. In more detail, the language (either <code>"en"</code>, <code>"nl"</code>, or <code>"gr"</code> for English, Dutch and Greek, respectively), genre (either <code>"articles"</code> or <code>"reviews"</code>), and the folder of each problem (relative path).</p>
	<pre class="prettyprint lang-py" style="overflow-x:auto">
[
   {"language": "en", "genre": "articles", "folder": "problem001"},
   ...
]
	</pre>
	<p>The ground truth data of the training corpus consists of two files for each clustering problem: <code>clustering.json</code> and <code>ranking.json</code> similar to the files described in the <strong>Output</strong> section (see details below). All ground truth files are located in the <code>truth</code> folder.</p>
	<p><a class="btn btn-default" target="_blank" href="http://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-16/pan16-data/pan16-author-clustering-training-dataset-2016-02-17.zip">Download corpus</a></p>
</div>
    </div>
    <div class="panel panel-default">
      <div class="panel-heading">Evaluation Phase</div>
      <div class="panel-body">Once you finished tuning your approach to achieve satisfying performance on the training corpus, your software will be tested on the evaluation corpus. During the competition, the evaluation corpus will not be released publicly. Instead, we ask you to <strong>submit your software</strong> for evaluation at our site as described below.
<br>After the competition, the evaluation corpus will become available including ground truth data. This way, you have all the necessities to evaluate your approach on your own, yet being comparable to those who took part in the competition.
</div></div>
    <div class="panel panel-default">
      <div class="panel-heading">Output</div>
      <div class="panel-body">
<p>Your system should produce <strong>two output files</strong> in <a href="http://www.json.org/">JSON</a>:</p>
<ul>
	<li>One output file including complete information about the detected clusters named <code>clustering.json</code>. Each cluster should contain all documents found in the collection by a specific author. A JSON file of the following format should be produced (a list of clusters, each cluster is a list of documents):<li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[
	[
		{"document":  "filename1"},
		{"document":  "filename2"},
	…
	],
…
]
</pre>

<p>The clusters should be non-overlapping, thus each filename should belong to exactly one cluster.</p>

<li>One output file named <code>ranking.json</code> including a list of document pairs ranked according to a real-valued score in [0,1], where higher values denote more confidence that the pair of documents are by the same author. A JSON file of the following format should be produced (a list of document pairs and a real-valued number):</li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[
	{"document1": "filename1",
	 "document2": "filename2",
	 "score": real-valued-number},
	…
]
</pre>
<p>The order of documents within a pair is not important (e.g. "document1": "filename1", "document2": "filename2" is the same with "document2": "filename1",
 "document1": "filename2"). In case the same pair is reported more than once the first occurrence will be taken into account.</p>
</ul>

<p>An <strong>illustrating example</strong> follows. Let’s assume that a document collection of 6 files is given: file1.txt, file2.txt, file3.txt, file4.txt, file5.txt, and file6.txt. There are 3 clusters: (i) file1.txt, file3.txt, and file4.txt are by the same author, (ii) file5.txt and file6.txt are by another author and (iii) file2.txt is by yet another author. </p>
<ul>
<li>The output file in JSON for the complete author clustering task should be:</li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[   [	{"document": "file1.txt"},
		{"document": "file3.txt"},
		{"document": "file4.txt"}	],
	[
		{"document": "file5.txt"},
		{"document": "file6.txt"}	],
	[
		{"document": "file2.txt"}	]
]
</pre>
<li>An example of the output file for authorship-link ranking could be:</li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[	{"document1": "file1.txt",
	 "document2": "file4.txt",
	 "score": 0.95},

	{"document1": "file3.txt",
	 "document2": "file4.txt",
	 "score": 0.75},

	{"document1": "file5.txt",
	 "document2": "file6.txt",
	 "score": 0.66},

	{"document1": "file1.txt",
	 "document2": "file3.txt",
	 "score": 0.63}
]
</pre>
</ul>
</div></div>
    <div class="panel panel-default">
      <div class="panel-heading">Performance Measures</div>
      <div class="panel-body">
      	<ul>
      		<li>The clustering output will be evaluated according to <strong>BCubed F-score</strong> (<a href="http://nlp.uned.es/docs/amigo2007a.pdf">Amigo et al. 2007</a>)</li>
      		<li>The ranking of authorship links will be evaluated according to <strong>Mean Average Precision</strong> (<a href="http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html">Manning et al. 2008</a>)</li>
 	</ul>
	</div></div>
	  <div class="panel panel-default">
      <div class="panel-heading">Submission</div>
      <div class="panel-body">
        <p>We ask you to prepare your software so that it can be executed via command line calls. The command shall take as input (i) an absolute path to the directory of the evaluation corpus and (ii) an absolute path to an empty output directory:</p>
	<pre class="prettyprint lang-py" style="overflow-x:auto">
> mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY
	</pre>
	<p>Within <code>EVALUATION-DIRECTORY</code> a <code>info.json</code> file and a number of folders, one for each clustering problem, will be found (similar to the training corpus as described above). For each clustering problem, a new folder should be built in <code>OUTPUT-DIRECTORY</code> using the same folder name found in <code>info.json</code> for that problem and within that folder the <code>clustering.json</code> and <code>ranking.json</code> output files should be written (similar to the <code>truth</code> folder of the training corpus).</p>
        <p>You can choose freely among the available programming languages and among the operating systems Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual machine that will be made accessible to you after registration. You will be able to reach the virtual machine via ssh and via remote desktop. More information about how to access the virtual machines can be found in the user guide below:</p>
        <p><a class="btn btn-default" href="pan15-virtual-machine-user-guide.pdf">PAN Virtual Machine User Guide »</a></p>
        <p>Once deployed in your virtual machine, we ask you to access TIRA at <a href="http://www.tira.io">www.tira.io</a>, where you can self-evaluate your software on the test data.</p>
        <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to grant us usage rights only for the purpose of the PAN competition. We agree not to share your software with a third party or use it for other purposes than the PAN competition.</p>
      </div>
    </div>
    

    <div class="panel panel-default">
      <div class="panel-heading">Related Work</div>
      <div class="panel-body">
        <p>We refer you to:</p>
        <ul><li>
			<a href="../../clef15/pan15-web/proceedings.html">PAN@CLEF'15</a></li>
			<li>
        	<a href="../../clef13/pan13-web/proceedings.html">PAN@CLEF'13</a>
        </li><li>
			<a href="../../clef14/pan14-web/proceedings.html">PAN@CLEF'14</a></li>
			<li>
        	<a href="../../clef13/pan13-web/proceedings.html">PAN@CLEF'13</a>
        </li><li>
        <a href="../../clef12/pan12-web/proceedings.html">PAN@CLEF'12</a>
        </li><li>
        <a href="../../clef11/pan11-web/proceedings.html">PAN@CLEF'11</a>
        </li><li>
        Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3, March 2008.
        </li><li>
        Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
        Authorship Attribution</a>. Journal of the American Society for Information Science and Technology, Volume 60, Issue 1, pages 9-26, January 2009.
        </li><li>
        Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A Survey of Modern Authorship Attribution Methods</a>.
        Journal of the American Society for Information Science and Technology, Volume 60, Issue 3, pages 538-556, March 2009.
        </li></ul>
      </div>
    </div>
    <div id="task-committee-clustering" class="row">
      <div class="col-xs-12">
        <h2 class="page-header">Task Chair</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.icsd.aegean.gr/lecturers/stamatatos/" target="_blank"><img src="../pan16-figures/stathis.jpg" class="img-rounded" alt="Efstathios Stamatatos"></a>
          <p><a href="http://www.icsd.aegean.gr/lecturers/stamatatos/" target="_blank">Efstathios Stamatatos</a></p>
          <p style="font-size:10pt">University of the Aegean</p>
        </div>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-12">
        <h2>Task Committee</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.clips.ua.ac.be/~walter/" target="_blank"><img src="../pan16-figures/walter.jpg" class="img-rounded" alt="Walter Daelemans"></a>
          <p style="white-space:nowrap"><a href="http://www.clips.ua.ac.be/~walter/" target="_blank">Walter Daelemans</a></p>
          <p style="font-size:10pt">University of Antwerp</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.mathcs.duq.edu/~juola/" target="_blank"><img src="../pan16-figures/patrick.jpg" class="img-rounded" alt="Patrick Juola"></a>
          <p style="white-space:nowrap"><a href="http://www.mathcs.duq.edu/~juola/" target="_blank">Patrick Juola</a></p>
          <p style="font-size:10pt">Duquesne University</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.uni-weimar.de/medien/webis/people" target="_blank"><img src="../pan16-figures/martin.jpg" class="img-rounded" alt="Martin Potthast"></a>
          <p style="white-space:nowrap"><a href="http://www.uni-weimar.de/medien/webis/people" target="_blank">Martin Potthast</a></p>
          <p style="font-size:10pt">Bauhaus-Universit&auml;t Weimar</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.webis.de" target="_blank"><img src="../pan16-figures/benno.jpg" class="img-rounded" alt="Benno Stein"></a>
          <p style="white-space:nowrap"><a href="http://www.webis.de" target="_blank">Benno Stein</a></p>
          <p style="font-size:10pt">Bauhaus-Universit&auml;t Weimar</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.clips.ua.ac.be/people/ben-verhoeven" target="_blank"><img src="../pan16-figures/ben.jpg" class="img-rounded" alt="Ben Verhoeven"></a>
          <p style="white-space:nowrap"><a href="http://www.clips.ua.ac.be/people/ben-verhoeven" target="_blank">Ben Verhoeven</a></p>
          <p style="font-size:10pt">University of Antwerp</p>
        </div>
      </div>
    </div>
  </div>

  <div class="col-sm-6">
    <h2 id="source-retrieval">Author Diarization <small>or Intrinsic Plagiarism Detection</small></h2>

      <p>The term <i>author diarization</i> originates from the research field <strong>speaker diarization</strong>, where approaches try to automatically identify and cluster different speakers of an audio speech signal like a telephone conversation or a political TV debate by processing and analyzing the audio frequency signal (an overview of approaches can be found, for example, <a href="http://www1.icsi.berkeley.edu/~vinyals/Files/taslp2011a.pdf">here</a>).</p>
      <p>Similar to such approaches, the task of <strong>author diarization</strong> in this PAN edition is to identify different authors within a single document. Such documents may be the result of a collaborative work (e.g., a combined master thesis written by two students, a scientific paper written by four authors, …), or the result of plagiarism. The latter is thereby a special case, where it can be assumed that the main text is written by one author and only some fragments are by other writers (the plagiarized or intrusive sections). On the other hand, the contributions of a collaboratively written document may be equally weighted, i.e., each author contributes to the same extent.</p>



      <div class="panel panel-default">
          <div class="panel-heading">Task</div>
          <div class="panel-body">

              <p>Given a document, identify and group text fragments that correspond to individual authors. Similarly to the situation in speaker diarization approaches, where active speakers may change at any time, you cannot assume that changes in authorship occur, for example, only on paragraph boundaries. But you should rather be prepared to detect different authors at any text position. An example could be as follows:</p>

              <p style="margin-left:30px"><i>"<span style="color:RoyalBlue">She is also a successful businesswoman and an American icon,</span><span style="color:red"> was born in Jersey City to middle-class Polish-American parents and she earned a partial scholarship to …</span>"</i></p>

              <p>Nevertheless, you may use paragraph boundaries or other useful metrics as heuristic to potential changes.</p>

              <p>To cover different variants of the problem, the task of this years PAN edition is split into three subproblems. For this year’s edition, all documents are provided in English.</p>

              <ul>
                  <li><p><strong>Traditional intrinsic plagiarism detection</strong>: here, you can assume that there exists one main author who wrote at least 70% of the text. Up to the other 30% may be written by other authors. For this problem, you should build exactly two clusters: one containing the text fragments of the main author, and the other one containing the intrusive fragments. </p></li>
                  <li><p><strong>Diarization with a given number (n) of authors</strong>: given a document, the task is to build exactly n clusters containing the contributions of the different writers. Thereby, each author may have contributed to an arbitrary, but non-zero extent.</p></li>
                  <li><p><strong>Diarization with an unknown number of authors</strong>: finally, this variant covers the most challenging task, which is similar to the previous task, but without the information of knowing how many authors contributed to the document.</p></li>
              </ul>
          </div>
      </div>


      <div class="panel panel-default">
          <div class="panel-heading">Training Phase</div>
          <div class="panel-body">
              <p>To develop your algorithms, training data sets for each sub problem including corresponding solutions are provided.</p>
              <p><br/>
              <a class="btn btn-default" target="_blank" href="http://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-16/pan16-data/pan16-author-diarization-training-dataset-2016-02-17.zip">Download corpus</a></p>
              <br/><p>The data set consists of three folders corresponding to each subtask. For each problem instance X in each subtask, three files are provided:</p>
              <ul>
                  <li><code>problem-X.txt</code> contains the actual text</li>
                  <li><code>problem-X.meta</code> contains meta information about the file in <a href="http://www.json.org/">JSON</a> format. It contains the<code>"language"</code> (which is always <strong>English</strong> this year), the problem <code>"type"</code> (<code>"plagiarism"</code> or <code>"diarization"</code>) and for the diarization problem with given number of authors additionally the correct number of authors (<code>"numAuthors"</code>)</li>
                  <li><code>problem-X.truth</code> contains the ground truth, i.e., the correct solution in <a href="http://www.json.org/">JSON</a> format:
<pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "authors": [
        [
            {"from": fromCharPosition,
            "to": toCharPosition},
            …
        ],
        …
    ]
}
</pre>

                      <p>
                          To identify the text fragments, the absolute character start/end positions within the document are used, whereby the document starts at character position 0.</p>

                      <p>
                          Note that for simplicity reasons the solutions for the <strong>intrinsic plagiarism task</strong> contains exactly 2 clusters: one for the main author and one combined for all other authors. Nevertheless, when producing the output, you are free to create as many clusters as you wish for the plagiarized sections.
                      </p>

                          <p>An example for an <strong>intrinsic plagiarism detection solution</strong> could look like this:
                          <pre class="prettyprint lang-py" style="overflow-x:auto">
{
	"authors": [
		[
			{"from": 314, "to": 15769}
		],
		[
			{"from": 0, "to": 313},
			{"from": 15770, "to": 19602}
		]
	]
}
</pre>

                          An example of the <strong>diarization solution</strong> of a document that was written by four authors could then look like this:</p>
<pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "authors": [
        [
            {"from": 123, "to": 400},
            {"from": 598, "to": 680}
        ],
        [
            {"from": 0, "to": 122}
        ],
        [
            {"from": 401, "to": 597},
            {"from": 681, "to": 1020},
            {"from": 1101, "to": 1400}
        ],
        [
            {"from": 1021, "to": 1100}
        ]
    ]
}
</pre>

                      <p>Of course, in the actual evaluation phase the ground truth, i.e., the <code>problem-X.truth</code> file will be missing.</p>
                  </li>
              </ul>
          </div>
      </div>

      <div class="panel panel-default">
          <div class="panel-heading">Evaluation Phase</div>
          <div class="panel-body">Once you finished tuning your approach to achieve satisfying performance on the training corpus, your software will be tested on the evaluation corpus. During the competition, the evaluation corpus will not be released publicly. Instead, we ask you to <strong>submit your software</strong> for evaluation at our site as described below.
              <br>After the competition, the evaluation corpus will become available including ground truth data. This way, you have all the necessities to evaluate your approach on your own, yet being comparable to those who took part in the competition.
          </div></div>


      <div class="panel panel-default">
          <div class="panel-heading">Output</div>
          <div class="panel-body">
             <p>
                 In general, the data structure during the evaluation phase will be similar to that in the training phase, with the exception that the ground truth files are missing.
                 This means, you can also use the information provided in the <code>problem-X.meta</code> file. Your software should finally output the missing solution file <code>problem-X.truth</code> for every problem instance X in the respective output folder (see Submission). The output syntax should thereby be exactly like it is used in the training phase.
             </p>

              <p>In general, there is no difference in the output between the intrinisic plagiarism detection and the diarization subtasks. Moreover, the <strong>order</strong> of the entries is <strong>not relevant</strong>.
                  <p>In the following, we provide you with some examples for both subtasks:</p>

              <ul>
                  <li>
              <p>
              For the <strong>intrinsic plagiarism detection</strong> subtask, you should create one entry for the main author. For the plagiarized sections you are free to either combine them into one entry (like it is done in the training data) or split them into more entries. As an example, if you found 2 plagiarized sections in the file <code>problem-3.txt</code>, you should produce the file <code>problem-3.truth</code>, where both</p>
<pre class="prettyprint lang-py" style="overflow-x:auto">
{
	"authors": [
		[
			{"from": 314, "to": 15769}
		],
		[
			{"from": 0, "to": 313},
			{"from": 15770, "to": 19602}
		]
	]
}
</pre>

              <p>and</p>

<pre class="prettyprint lang-py" style="overflow-x:auto">
{
	"authors": [
		[
			{"from": 314, "to": 15769}
		],
		[
			{"from": 0, "to": 313}
        ],
			{"from": 15770, "to": 19602}
		]
	]
}
</pre>
              <p>are valid solutions.</p>

                  </li>
                  <li>
              <p>
                  For the <strong>diarization</strong> subtask, if you found 3 authors for the file <code>problem-12.txt</code>, you should produce the file <code>problem-12.truth</code> containing the solution like this:</p>
                  <pre class="prettyprint lang-py" style="overflow-x:auto">
{
	"authors": [
		[
			{"from": 0, "to": 409},
			{"from": 645, "to": 4893}
		],
		[
			{"from": 410, "to": 644},
			{"from": 4894, "to": 6716}
		],
		[
			{"from": 6717, "to": 15036}
		]
	]
}
</pre>
                  </li>
              </ul>


          </div>
      </div>


      <div class="panel panel-default">
          <div class="panel-heading">Performance Measures</div>
          <div class="panel-body">
              <ul>
                  <li>
                      To evaluate the quality of the intrinsic plagiarism detection algorithms, the <strong>micro- and macro-averaged F-score</strong> will be used (see <a href="../../clef11/pan11-web/plagiarism-detection.html">PAN@CLEF'11</a>, or <a href="http://www.uni-weimar.de/medien/webis/publications/papers/stein_2010p.pdf#page=2">this paper</a>) will be used.
                  </li>
                  <li>For the diarization algorithms, the <strong>BCubed F-score</strong> (<a href="http://nlp.uned.es/docs/amigo2007a.pdf">Amigo et al. 2007</a>) will be used.</li>
              </ul>
          </div>
      </div>

      <div class="panel panel-default">
          <div class="panel-heading">Submission</div>
          <div class="panel-body">
              <p>We ask you to prepare your software so that it can be executed via command line calls. The command shall take as input (i) an absolute path to the directory of the evaluation corpus and (ii) an absolute path to an empty output directory:</p>
<pre class="prettyprint lang-py" style="overflow-x:auto">
> mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY
</pre>
              <p>Within <code>EVALUATION-DIRECTORY</code>, you will find a list of folders. Exactly as in the training data, each folder then contains <code>[filename].txt</code> and corresponding <code>[filename].meta</code> files. You should use the latter to determine which problem type you should solve (plagiarism detection or diarization).</p>
              <p>For each folder, you should produce a folder in the <code>OUTPUT-DIRECTORY</code> with the same name. Finally, your software should write the <code>[filename].truth</code> file for each <code>[filename]</code> inside these directories.</p>
              <p>You can choose freely among the available programming languages and among the operating systems Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual machine that will be made accessible to you after registration. You will be able to reach the virtual machine via ssh and via remote desktop. More information about how to access the virtual machines can be found in the user guide below:</p>
              <p><a class="btn btn-default" href="pan15-virtual-machine-user-guide.pdf">PAN Virtual Machine User Guide »</a></p>
              <p>Once deployed in your virtual machine, we ask you to access TIRA at <a href="http://www.tira.io">www.tira.io</a>, where you can self-evaluate your software on the test data.</p>
              <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to grant us usage rights only for the purpose of the PAN competition. We agree not to share your software with a third party or use it for other purposes than the PAN competition.</p>
          </div>
      </div>


      <div class="panel panel-default">
          <div class="panel-heading">Related Work</div>
          <div class="panel-body">
              <p>We refer you to:</p>
              <ul><li>
                  <a href="../../clef11/pan11-web/proceedings.html">PAN@CLEF'11</a>
              </li><li>
                  <a href="../../clef12/pan12-web/proceedings.html">PAN@CLEF'12</a>
              </li>
                  <li>Sven Meyer zu Eissen, Benno Stein. <a href="http://www.uni-weimar.de/medien/webis/publications/papers/stein_2006d.pdf">Intrinsic Plagiarism Detection</a>. In Advances in Information Retrieval. Proceedings of the 28th European Conference on IR Research (ECIR), pages 565-569, 2006</li>
                  <li>
                      Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3, March 2008.
                  </li><li>
                      Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A Survey of Modern Authorship Attribution Methods</a>.
                      Journal of the American Society for Information Science and Technology, Volume 60, Issue 3, pages 538-556, March 2009.
                  </li></ul>
          </div>
      </div>



      <div id="task-committee-diarization" class="row">
      <div class="col-xs-12">
        <h2 class="page-header">Task Chair</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="https://dbis-informatik.uibk.ac.at/1-1-Home.html" target="_blank"><img src="../pan16-figures/michael.png" class="img-rounded" alt="Michael Tschuggnall"></a>
          <p><a href="https://dbis-informatik.uibk.ac.at/1-1-Home.html" target="_blank">Michael Tschuggnall</a></p>
          <p style="font-size:10pt">University of Innsbruck</p>
        </div>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-12">
        <h2>Task Committee</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="https://dbis-informatik.uibk.ac.at/1-1-Home.html" target="_blank"><img src="../pan16-figures/guenther.jpg" class="img-rounded" alt="G&uuml;nther Specht"></a>
          <p><a href="https://dbis-informatik.uibk.ac.at/1-1-Home.html" target="_blank">G&uuml;nther Specht</a></p>
          <p style="font-size:10pt">University of Innsbruck</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.uni-weimar.de/medien/webis/people" target="_blank"><img src="../pan16-figures/martin.jpg" class="img-rounded" alt="Martin Potthast"></a>
          <p style="white-space:nowrap"><a href="http://www.uni-weimar.de/medien/webis/people" target="_blank">Martin Potthast</a></p>
          <p style="font-size:10pt">Bauhaus-Universit&auml;t Weimar</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.webis.de" target="_blank"><img src="../pan16-figures/benno.jpg" class="img-rounded" alt="Benno Stein"></a>
          <p style="white-space:nowrap"><a href="http://www.webis.de" target="_blank">Benno Stein</a></p>
          <p style="font-size:10pt">Bauhaus-Universit&auml;t Weimar</p>
        </div>
      </div>
    </div>
  </div>
</div>

<footer>
  <p class="pull-right">© pan.webis.de</p>
</footer>

</div> <!-- /container -->

<script src="../../js/jquery.js"></script>
<script src="../../js/bootstrap.min.js"></script>
<script src="../../js/prettify.js"></script>
<script>
  !function ($) {
    $(function(){
      window.prettyPrint && prettyPrint()   
    })
  }(window.jQuery)
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-70770005-1', 'auto');
  ga('send', 'pageview');

</script>

</body>
</html>

