---
layout: default
nav_active: tasks
title: PAN @ CLEF 2018 - Author Identification
description: PAN @ CLEF 2018 - Author Identification
---

<main>
    <header class="uk-section uk-section-muted">

        <nav class="uk-container">
            <div class="uk-align-right uk-visible@m uk-text-muted">
                <a class="uk-button" href="{{ 'clef17/pan17-web/author-identification.html' | relative_url }}"
                   data-uk-icon="chevron-left"></a>
                CLEF 2018
                <a class="uk-button uk-padding-remove-right" href="{{ 'clef19/pan19-web/author-identification.html' | relative_url }}"
                   data-uk-icon="chevron-right"></a>
            </div>

            <ul class="uk-text-muted uk-tab uk-margin-remove-top">
                <li><a href="{{ '/clef18/pan18-web/index.html' | relative_url }}">Overview</a></li>
                <li><a href="{{ '/clef18/pan18-web/submission.html' | relative_url }}">Submission</a></li>
                <li class="uk-active">
                    <a href="#">Tasks <span class="uk-margin-small-left"
                                            data-uk-icon="icon: chevron-down"></span></a>
                    <div class="uk-dropdown" data-uk-dropdown="mode: click">
                        <ul class="uk-nav uk-dropdown-nav">
                            <li class="uk-active"><a
                                    href="{{ '/clef18/pan18-web/author-identification.html' | relative_url }}">Author
                                Identification</a></li>
                            <li><a href="{{ '/clef18/pan18-web/author-profiling.html' | relative_url }}">Author
                                Profiling</a></li>
                            <li><a href="{{ '/clef18/pan18-web/author-obfuscation.html' | relative_url }}">Author
                                Obfuscation</a></li>
                        </ul>
                    </div>
                </li>
                <li><a href="{{ '/clef18/pan18-web/program.html' | relative_url }}">Program</a></li>
                <li><a href="{{ '/clef18/pan18-web/proceedings.html' | relative_url }}">Proceedings</a></li>
            </ul>
        </nav>

        <div class="uk-container uk-margin-small">
            {% include current-register-quicklink.html year=2018 %}
            <h1 class="uk-margin-small">Author Identification</h1>
            <div class="page-header-meta">CLEF @ PAN 2018</div>
        </div>

        <div class="uk-container uk-margin-medium">
            <p class="uk-text-lead">This task is divided into <a href="#cross-domain">cross-domain authorship
                attribution</a> and
                <a href="#style-change-detection">style change detection</a>. You can choose to solve one or both of
                them.</p>
        </div>
    </header>

    <section>
        <div class="uk-section">
            <div class="uk-container">
                <div class="uk-column-1-2@l uk-text-justify">
                    <h2 id="cross-domain">Cross-domain Authorship Attribution</h2>
                    <p>Authorship attribution is an important problem in information retrieval and computational linguistics but
                        also in applied areas such as law and journalism where knowing the author of a document (such as a ransom
                        note) may enable e.g. law enforcement to save lives. The most common framework for testing candidate
                        algorithms is the <strong>closed-set attribution</strong> task: given a sample of reference documents from a
                        restricted and finite set of candidate authors, the task is to determine the most likely author of a
                        previously unseen document of unknown authorship? This task may be quite challenging when documents of known
                        and unknown authorship come from different domains (e.g., thematic area, genre).
                    <p>In this edition of PAN, for the first time, we focus on <strong>cross-domain attribution</strong>applied to
                        <strong>Fanfiction</strong>. Fanfiction refers to fictional forms of literature which are nowadays produced
                        by admirers ('fans') of a certain author (e.g. J.K. Rowling), novel ('Pride and Prejudice'), TV series
                        (Sherlock Holmes), etc. The fans heavily borrow from the original work's theme, atmosphere, style,
                        characters, story world etc. to produce new fictional literature, i.e. the so-called
                        <strong>fanfics</strong>. This is why fanfiction is also known as transformative literature and has
                        generated a number of controversies in recent years related to the intellectual rights property of the
                        original authors (cf. plagiarism). Fanfiction, however, is typically produced by fans without any explicit
                        commercial goals. The publication of fanfics typically happens online, on informal community platforms that
                        are dedicated to making such literature accessible to a wider audience (e.g. <a
                                href=https://www.fanfiction.net>fanfiction.net</a>). The original work of art or genre is typically
                        refered to as a <strong>fandom</strong>.</p>
                    <p>The cross-domain attribution task in this edition of PAN can be more accurately described as
                        <strong>cross-fandom
                            attribution in fanfiction</strong>. In more detail, all documents of unknown authorship are fanfics of the
                        same fandom (target fandom) while the documents of known authorship by the candidate authors are fanfics of
                        several fandoms (other than the target-fandom).</p>

                    <h3>Task</h3>
                    Given a set of documents (known fanfics) by a small number (up to 20) of candidate authors, identify the
                    authors of another set of documents (unknown fanfics). Each candidate author has contributed at least one of
                    the unknown fanfics, which all belong to the same target fandom. The known fanfics belong to several fandoms
                    (excluding the target fandom), although not necessarily the same for all candidate authors. An equal number
                    of fanfics per candidate author is provided. In contrast, the unknown fanfics are not equally distributed
                    over the authors. The text-length of fanfics varies from 500 to 1,000 tokens. All documents are in the same
                    language that may be <strong>English, French, Italian, Polish, or Spanish</strong>.

                    <h3>Development Phase</h3>
                    <p>To develop your software, we provide you with a corpus with highly similar characteristics to the
                        evaluation corpus. It comprises a set of cross-domain authorship attribution problems in each of the
                        following 5 languages: English, French, Italian, Polish, and Spanish. Note that we specifically avoid to
                        use the term 'training corpus' because <strong>the sets of candidate authors of the development and the
                        evaluation corpora are not overlapping</strong>. Therefore, your approach should not be designed to
                        particularly handle the candidate authors of the development corpus. </p>
                    <p>Each problem consists of a set of known fanfics by each candidate author and a set of unknown fanfics
                        located in separate folders. The file <code>problem-info.json</code> that can be found in the main
                        folder of each problem, shows the name of folder of unknown documents and the list of names of candidate
                        author folders. </p>

                    <pre class="prettyprint"><code class="lang-json">{
    "unknown-folder": "unknown",
    "candidate-authors": [
        { "author-name": "candidate00001" },
        { "author-name": "candidate00002" },
        ...
    ]
}</code></pre>
                    <p>The true author of each unknown document can be seen in the file <code>ground-truth.json</code>, also found in
                        the main folder of each problem.</p>
                    <p>In addition, to handle a collection of such problems, the file <code>collection-info.json</code>includes
                        all relevant information. In more detail, for each problem it lists its main folder, the language
                        (either <code>"en"</code>, <code>"fr"</code>, <code>"it"</code>, <code>"pl"</code>, or
                        <code>"sp"</code>)
                        and encoding (always <code>UTF-8</code>) of its documents. </p>
                    <pre class="prettyprint"><code class="lang-json">[
    { "problem-name": "problem00001",
      "language": "en",
      "encoding": "UTF-8" },
    { "problem-name": "problem00002",
       "language": "fr",
       "encoding": "UTF-8" },
	  ...
]</code></pre>

                    <p><a class="uk-button uk-button-primary" target="_blank"
                          href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-18/pan18-data/pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02-password-protected.zip">Download
                        corpus</a></p>
                    <p>This is a password-protected file. To obtain the password, first
                        <a href=https://docs.google.com/forms/d/e/1FAIpQLSfR_xoBuGU3q7o3EYPoItN28UPuZENjs3wlWYEX_EdRGUyRfA/viewform>register</a>
                        for the author identification task at PAN-2018, and then
                        <a href="mailto:pan@webis.de?Subject=PAN-18" target="_top">notify</a> PAN organizers.</p>
                    <p>In addition, a language-independent baseline approach based on a character n-gram representation and a
                        linear SVM classifier is provided.</p>
                    <p><a class="uk-button uk-button-primary" target="_blank"
                          href="../pan18-code/pan18-cdaa-baseline.py">Download baseline</a></p>

                    <div style="break-inside: avoid">
                        <h3>Evaluation Phase</h3>
                        <p>Once you finished tuning your approach to achieve satisfying performance on the development corpus, your
                            software will be tested on the evaluation corpus. During the competition, the evaluation corpus will not
                            be released publicly. Instead, we ask you to <strong>submit your software</strong> for evaluation at our
                            site as described below.</p>
                    </div>
                    <p>After the competition, the evaluation corpus will become available including ground truth data.
                        This way, you have all the necessities to evaluate your approach on your own, yet being comparable to
                        those who took part in the competition. </p>
                    <p><a class="uk-button uk-button-primary" target="_blank"
                          href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-18/pan18-data/pan18-cross-domain-authorship-attribution-test-dataset2-2018-04-20-password-protected.zip">Download
                        evaluation corpus</a></p>
                    <p>This is a password-protected file (use the same password as for the development corpus). </p>


                    <h3>Output</h3>
                    <div class="panel-body">
                        <p>Your system should produce one output file for each authorship attribution problem in <a
                                href="http://www.json.org/">JSON</a>. The name of the output files should be <code>answers-PROBLEMNAME.json</code>
                            (e.g., <code>answers-problem00001.json</code>, <code>answers-problem00002.json</code>)
                            including the list of unknown documents and their predicted author:</p>

                        <pre class="prettyprint"><code class="lang-json">[
    { "unknown-text":  "unknown00001.txt", 
      "predicted-author":  "candidate00003" },
    { "unknown-text":  "unknown00002.txt", 
      "predicted-author":  "candidate00005" },
	...
]</code></pre>
                        <h3>Performance Measures</h3>
                        <p>The submissions will be evaluated in each attribution problem separately based on their
                            <strong>macro-averaged F1 score</strong>. Participants will be ranked according to their
                            average macro-F1 across all attribution problems of the evaluation corpus. </p>
                        <p>We provide you with a Python script that calculates macro-F1 (and optionally the confusion
                            matrix) of a single attribution problem:</p>
                        <p><a class="uk-button uk-button-primary" target="_blank"
                              href="../pan18-code/pan18-cdaa-evaluator-single.py">Download evaluation script</a></p>
                        <p>and another Python script that calculates macro-F1 for a collection of attribution problems:</p>
                        <p><a class="uk-button uk-button-primary" target="_blank"
                              href="../pan18-code/pan18-cdaa-evaluator.py">Download evaluation script</a></p>

                        <h3>Submission</h3>
                        <p>We ask you to prepare your software so that it can be executed via command line calls. The
                            command shall take as input (i) an absolute path to the directory of the evaluation corpus and (ii)
                            an absolute path to an existing empty output directory:</p>
                        <pre class="prettyprint"><code class="lang-bash">mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY</code></pre>
                        <p>Within <code>EVALUATION-DIRECTORY</code> a <code>collection-info.json</code> file and a number of
                            folders, one for each attribution problem, will be found (similar to the development corpus
                            as described above). For each attribution problem, the output file should be written in
                            <code>OUTPUT-DIRECTORY</code>.
                        </p>
                        <p><strong>Note:</strong> Each attribution problem should be solved independently of other
                            problems in the collection.</p>
                        <p>You can choose freely among the available programming languages and among the operating
                            systems Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual
                            machine that will be made accessible to you after registration. You will be able to reach the
                            virtual machine via ssh and via remote desktop. More information about how to access the virtual
                            machines can be found in the user guide below:</p>
                        <p><a class="uk-button uk-button-primary"
                              href="http://www.tira.io/static/tira-vm-user-guide.pdf">PAN Virtual Machine User Guide »</a></p>
                        <p>Once deployed in your virtual machine, we ask you to access TIRA at <a
                                href="http://www.tira.io">www.tira.io</a>,
                            where you can self-evaluate your software on the test data.</p>
                        <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to
                            grant us usage rights only for the purpose of the PAN competition. We agree not to share your
                            software with a third party or use it for other purposes than the PAN competition.</p>

                        <h3>Related Work</h3>
                        <p>We refer you to:</p>
                        <ul class="uk-list uk-list-bullet">
                            <li>
                                <a href="http://pan.webis.de/clef12/pan12-web/author-identification.html">Author
                                    identification task at PAN@CLEF'12</a> (closed-set authorship attribution)
                            </li>
                            <li>
                                <a href="http://pan.webis.de/clef11/pan11-web/author-identification.html">Author
                                    identification task at PAN@CLEF'11</a> (closed-set authorship attribution)
                            </li>
                            <li>
                                Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                                Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3, March 2008.
                            </li>
                            <li>
                                Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                                    href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
                                Authorship Attribution</a>. Journal of the American Society for Information Science and
                                Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                            </li>
                            <li>
                                Efstathios Stamatatos. <a
                                    href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                                Survey of Modern Authorship Attribution Methods</a>.
                                Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                                pages 538-556, March 2009.
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>  <!-- section -->

        <div class="uk-section uk-section-muted">
            <div class="uk-container">

                <h2>Task Chair</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/kestemont.html %}
                </div>

                <h2>Task Committee</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/stamatatos.html %}
                    {% include people-cards/daelemans.html %}
                    {% include people-cards/potthast.html %}
                    {% include people-cards/stein.html %}
                </div>
            </div>
        </div>
    </section>

    <section>
        <div class="uk-section">
            <div class="uk-container">
                <div class="uk-column-1-2@l uk-text-justify">
                    <h2 id="style-change-detection">Style Change Detection</h2>

                    <p>While many approaches target the problem of identifying authors of whole documents, research on
                        investigating
                        multi-authored documents is sparse. In the last two PAN editions we therefore aimed to narrow
                        the gap by at first proposing a task to cluster by authors inside documents (<a
                                href="../../clef16/pan16-web/author-identification.html">Author Diarization, 2016</a>).

                        Relaxing the problem, the follow-up task (<a
                                href="../../clef17/pan17-web/author-identification.html">Style
                            Breach Detection, 2017</a>) focused only on identifying <i>style breaches</i>, i.e., to find
                        text positions where the authorship and thus the style changes. Nevertheless, the results of
                        participants revealed relatively low accuracies and indicated that this task is still too hard to tackle.

                    <p>Consequently, this year we propose a <em>substantially simplified task</em>, while still being a
                        continuation of last year's task: The only question that should be answered by participants is whether there
                        <em>exists</em> a style change in a given document or not. Further, we changed the name to <em>Style
                            <strong>Change</strong> Detection</em>, in order to reflect the task more intuitively.
                    </p>

                    Given a document, participants thus should apply intrinsic analyses to decide if the document is written
                    by one or more authors, i.e., if there exist style changes. While the precedent task demanded to
                    specifically locate the exact position of such changes, this year we only ask for a <b>binary
                    answer</b> per document:
                    <ul class="uk-list uk-list-bullet">
                        <li>
                            <code>yes</code>: the document contains at least one style change (is written by at least
                            two
                            authors)
                        </li>
                        <li><code>no</code>: the document has no style changes (is written by a single author)</li>
                    </ul>


                    <p>In this sense it is irrelevant to identify the number of style changes, the specific positions,
                        or to build clusters of authors. You may adapt existing algorithms other problem types such as
                        <em>intrinsic plagiarism detection</em> or <em>text segmentation</em>. For example, if you
                        already have an intrinsic plagiarism detection system, you can apply your method on this task by outputting
                        <code>yes</code> if you
                        found a plagiarism case or <code>no</code> otherwise (please note that intrinsic plagiarism
                        detection methods may need adaptions as they naturally are not designed to handle uniformly distributed
                        author texts).
                    </p>

                    <p>The following figure illustrates some possible scenarios and the expected output:</p>
                    <p><img src="../pan18-figures/style-change-sample.png" alt="Text Alignment"></p>

                    <h3>Task</h3>
                    <p>Given a document, determine whether it contains style changes or not, i.e., if it was written by a
                        single or multiple authors.</p>
                    <p>All documents are provided in English and may contain zero up to arbitrarily many style changes.</p>

                    <h3>Development Phase</h3>
                    <p>To develop your algorithms, a data set including corresponding solutions is provided:</p>
                    <ul class="uk-list uk-list-bullet">
                        <li>a <strong>training</strong> set: contains 50% of the whole dataset and includes solutions.
                            Use this set to feed/train your models.
                        </li>
                        <li>a <strong>validation</strong> set: contains 25% of the whole dataset and includes solutions.
                            Use this set to evaluate and optimize your models.
                        </li>
                        <li>a <strong>test</strong> set: contains 25% of the whole dataset and does not include
                            solutions. This set is used for evaluation (see later).
                        </li>
                    </ul>

                    <p>The whole data set is based on user posts from various sites of the
                        <a href="https://stackexchange.com">StackExchange network</a>, covering different topics and
                        containing approximately 300 to 1000 tokens per document. Moreover and for your convenience,
                        within the training/validation data set the exact locations of all style changes are provided,
                        as they may be helpful to develop your algorithms.</p>

                    <ul class="uk-list list-inline">
                        <li>
                            <a class="uk-button uk-button-primary" target="_blank"
                               href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-18/pan18-data/pan18-style-change-detection-training-dataset-2018-01-31.zip">Download
                                training set</a>
                        </li>
                        <li>
                            <a class="uk-button uk-button-primary" target="_blank"
                               href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-18/pan18-data/pan18-style-change-detection-validation-dataset-2018-01-31.zip">Download
                                validation set</a>
                        </li>
                        <li>
                            <a class="uk-button uk-button-primary" target="_blank"
                               href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-18/pan18-data/pan18-style-change-detection-test-dataset-2018-01-31.zip">Download
                                test set</a>
                        </li>
                    </ul>

                    <p>Additionally, we provide detailed meta data about each problem of all data sets, including
                        topics/subtopics, author distributions or average segment lengths.</p>

                    <p><a class="uk-button uk-button-primary" target="_blank"
                          href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-18/pan18-data/pan18-style-change-detection-meta-data-2018-06-21.zip">Download
                        meta data</a></p>

                    <p>For each problem instance X, two files are provided:</p>
                    <ul class="uk-list uk-list-bullet">
                        <li><code>problem-X.txt</code> contains the actual text</li>
                        <li><code>problem-X.truth</code> contains the ground truth, i.e., the correct solution in
                            <a href="http://www.json.org/">JSON</a> format:
                            <pre class="prettyprint"><code class="lang-json">{
    "changes": true/false,
    "positions": [
        character_position_change_1,
        character_position_change_2,
        ...
    ]
}</code></pre>
                            <p>
                                If present, the absolute character positions of the first non-whitespace character of
                                the new segment is provided in the solution (<code>positions</code>). Please note that this
                                information is only for development purposes and not used for the evaluation.</p>
                        </li>
                    </ul>

                    <div style="break-inside: avoid">
                        <h3>Evaluation Phase</h3>
                        <p>Once you finished tuning your approach to achieve satisfying performance on the training
                            corpus, your software will be tested on the evaluation corpus (test data set). You can expect the
                            test data set to be similar to the validation data set, i.e., also based on StackExchange user
                            posts and of similar size as the validation set. During the competition, the evaluation corpus will not
                            be released publicly. Instead, we ask you to <strong>submit your software</strong> for evaluation at our
                            site as described below.
                            After the competition, the evaluation corpus will become available including ground truth data.
                            This way, you have all the necessities to evaluate your approach on your own, yet being
                            comparable to
                            those who took part in the competition.</p>
                    </div>
                    <h3>Output</h3>
                    <p>In general, the data structure during the evaluation phase will be similar to that in the
                        training phase, with the exception that the ground truth files are missing. Thus, for each given problem
                        <code>problem-X.txt</code> your software should output the missing solution file
                        <code>problem-X.truth</code>.
                        The output should be a JSON object containing of a single property:</p>

                    <pre class="prettyprint"><code class="lang-json">{
    "changes": true/false
}</code></pre>
                    <p>Output <code>"changes" : true</code> if there are style changes in the document, and <code>"changes"
                        :
                        false</code> otherwise.</p>


                    <h3>Performance Measures</h3>
                    <p>The performance of the approaches will simply be measured and ranked by computing the
                        <strong>accuracy</strong>.</p>
                    <p>For your convenience, we provide the evaluator script written in Python.</p>
                    <p><a class="uk-button uk-button-primary" href="../pan18-code/pan18_scd_evaluator.py">Download
                        script</a>
                    </p>
                    <p>It takes three parameters: an input directory (the data set), an inputRun directory (your
                        computed predictions) and an output directory where the results file is written to. In addition
                        to the
                        <code>accuracy</code>achieved over the whole input directory, also an
                        <code>accuracy_solved</code> is computed that
                        considers only solved problem instances (i.e., if you only solved two problem instances and were
                        correct both times, the <code>accuracy_solved</code> would be 100%). Please note that this
                        measure is only for developing purposes and that <code>accuracy</code> over all items will be
                        used for the final evaluation.</p>

                    <h3>Submission</h3>
                    <p>We ask you to prepare your software so that it can be executed via command line calls. The command
                        shall take as input (i) an absolute path to the directory of the evaluation corpus and (ii) an
                        absolute path to an empty output directory:</p>
                    <pre class="prettyprint"><code class="lang-bash">mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY</code></pre>

                    <p>Within <code>EVALUATION-DIRECTORY</code>, you will find a list of problem instances, i.e., <code>[filename].txt</code>files.
                        For each problem instance you should produce the solution file <code>[filename].truth</code> in
                        the<code>OUTPUT-DIRECTORY</code> For instance, you
                        read<code>EVALUATION-DIRECTORY/problem-12.txt</code>,
                        process it and write your results to <code>OUTPUT-DIRECTORY/problem-12.truth</code>.</p>

                    <p>You can choose freely among the available programming languages and among the operating systems
                        Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual machine
                        that will be made accessible to you after registration. You will be able to reach the virtual machine
                        via ssh and via remote desktop. More information about how to access the virtual machines can be
                        found in the user guide below:</p>
                    <p><a class="uk-button uk-button-primary"
                          href="../../clef15/pan15-web/pan15-virtual-machine-user-guide.pdf">PAN Virtual Machine User
                        Guide</a>
                    </p>
                    <p>Once deployed in your virtual machine, we ask you to access TIRA at <a href="http://www.tira.io">www.tira.io</a>,
                        where you can self-evaluate your software on the test data.</p>

                    <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to grant us
                        usage rights only for the purpose of the PAN competition. We agree not to share your software
                        with a third party or use it for other purposes than the PAN competition.</p>

                    <h3>Related Work</h3>
                    <p>We refer you to:</p>
                    <ul class="uk-list uk-list-bullet">
                        <li>
                            <a href="../../clef17/pan17-web/proceedings.html">PAN@CLEF'17</a> (<i>Overview of the Author
                            Identification Task at PAN-2017</i> and <i>Style Breach Detection</i> section)
                        </li>
                        <li>
                            <a href="../../clef16/pan16-web/proceedings.html">PAN@CLEF'16</a> (<i>Clustering by
                            Authorship
                            Within and Across Documents</i> and <i>Author Diarization</i> section)
                        </li>
                        <li>Marti A. Hearst. <a href="http://anthology.aclweb.org/J/J97/J97-1003.pdf">TextTiling:
                            Segmenting Text into Multi-paragraph Subtopic Passages.</a>. In Computational Linguistics, Volume 23,
                            Issue 1, pages 33-64, 1997.
                        </li>
                        <li>Benno Stein, Nedim Lipka and Peter Prettenhofer.
                            <a href="https://www.uni-weimar.de/medien/webis/publications/papers/stein_2011a.pdf">Intrinsic
                                Plagiarism Analysis</a>. In Language Resources and Evaluation, Volume 45, Issue 1, pages
                            63–82, 2011.
                        </li>
                        <li>
                            Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                            Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                            March 2008.
                        </li>
                        <li>
                            Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                            Survey of Modern Authorship Attribution Methods</a>.
                            Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                            pages 538-556, March 2009.
                        </li>
                    </ul>
                </div>
            </div>
        </div>  <!-- section -->

        <div class="uk-section uk-section-muted">
            <div class="uk-container">

                <h2>Task Chair</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/tschuggnall.html %}
                </div>

                <h2>Task Committee</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/specht.html %}
                    {% include people-cards/potthast.html %}
                    {% include people-cards/stein.html %}
                </div>
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/pan18-organizations-section.html %}
            </div>
        </div>
    </section>
</main>
