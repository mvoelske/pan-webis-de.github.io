<!DOCTYPE html>
<html lang="en">
<head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>PAN 2018</title>

<link href="../../css/bootstrap.min.css" rel="stylesheet" />
<link href="../../css/prettify.css" rel="stylesheet" />

<style>
.navbar .navbar-nav {
  font-weight: bold;
}
</style>

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
  <script src="../../js/html5shiv.js"></script>
  <script src="../../js/respond.min.js"></script>
<![endif]-->

<link rel="shortcut icon" href="../pan18-figures/pan-icon-16x16.ico">
<!--
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
<link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
-->

</head>
<body>

<nav class="navbar navbar-inverse navbar-static-top" role="navigation">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="../../index.html"><img src="../pan18-figures/pan-logo-small-lightgrey.png" alt="PAN" style="margin-top:-5px"></a>
  </div>
  <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
    <ul class="nav navbar-nav navbar-right">
      <li><a href="../../index.html">Home</a></li>
      <li><a href="index.html">PAN @ CLEF 2018:</a></li>
      <li><a href="about.html">About</a></li>
      <li><a href="proceedings.html">Proceedings</a></li>
      <li class="dropdown active">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Tasks <span class="caret"></span></a>
        <ul class="dropdown-menu">
          <li class="active"><a href="author-identification.html">Author Identification</a></li>
          <li><a href="author-profiling.html">Author Profiling</a></li>
          <li><a href="author-obfuscation.html">Author Obfuscation</a></li>
        </ul>
      </li>
    </ul>
  </div>
</nav>

<div class="container">

<div class="row">
  <div class="col-xs-12">
    <h1 id="task-description" class="page-header">
      Author Identification
      <div class="btn-group">
        <a role="button" class="btn btn-default" href="../../clef17/pan17-web/author-identification.html"><span class="glyphicon glyphicon-chevron-left" style="color:grey;"></span></a>
        <span role="button" class="btn btn-default" disabled="disabled">2018</span>
        <span role="button" class="btn btn-default" disabled="disabled"><span class="glyphicon glyphicon-chevron-right" style="color:grey;"></span></span>
      </div>
    </h1>
    <!--
    <div class="thumbnail pull-right" style="text-align:right;margin-left:15px;"><a href="http://www.adobe.com/" target="_blank"><img src="../pan18-figures/logo-adobe.png" alt="Adobe" style="max-height:150px"></a><div style="font-size:7pt;margin-right:13px;margin-top:2px;">Sponsor</div></div>
    -->
    <p class="lead">This task is divided into <strong>cross-domain authorship attribution</strong> and <strong>style change detection</strong>. You can choose to solve one or both of them.</p>
    <ul class="nav nav-pills visible-xs">
      <li><a href="#author-clustering" class="btn btn-large"><span class="glyphicon glyphicon-chevron-down"></span> Author Clustering</a></li>
      <li><a href="#author-diarization" class="btn btn-large"><span class="glyphicon glyphicon-chevron-down"></span> Style Breach Detection</a></li>
    </ul>
  </div>
</div>

<div class="row">
  <div class="col-sm-6">
    <h2 id="author-clustering">Cross-domain Authorship Attribution</h2>
   <p>Authorship attribution is an important problem in information retrieval and computational linguistics but also in applied areas such as law and journalism where knowing the author of a document (such as a ransom note) may enable e.g. law enforcement to save lives. The most common framework for testing candidate algorithms is the <strong>closed-set attribution</strong> task: given a sample of reference documents from a restricted and finite set of candidate authors, the task is to determine the most likely author of a previously unseen document of unknown authorship? This task may be quite challenging when documents of known and unknown authorship come from different domains (e.g., thematic area, genre). 
	<p>In this edition of PAN, for the first time, we focus on <strong>cross-domain attribution</strong> applied to <strong>Fanfiction</strong>. Fanfiction refers to fictional forms of literature which are nowadays produced by admirers ('fans') of a certain author (e.g. J.K. Rowling), novel ('Pride and Prejudice'), TV series (Sherlock Holmes), etc. The fans heavily borrow from the original work's theme, atmosphere, style, characters, story world etc. to produce new fictional literature, i.e. the so-called <strong>fanfics</strong>. This is why fanfiction is also known as transformative literature and has generated a number of controversies in recent years related to the intellectual rights property of the original authors (cf. plagiarism). Fanfiction, however, is typically produced by fans without any explicit commercial goals. The publication of fanfics typically happens online, on informal community platforms that are dedicated to making such literature accessible to a wider audience (e.g. <a href=https://www.fanfiction.net>fanfiction.net</a>). The original work of art or genre is typically refered to as a <strong>fandom</strong>.</p>
	<p>The cross-domain attribution task in this edition of PAN can be more accurately described as <strong>cross-fandom attribution in fanfiction</strong>. In more detail, all documents of unknown authorship are fanfics of the same fandom (target fandom) while the documents of known authorship by the candidate authors are fanfics of several fandoms (other than the target-fandom). </p>    
<!--
<div class="panel panel-default">
      <div class="panel-heading">Task</div>
      <div class="panel-body">Given a collection of (up to 50) short documents (paragraphs extracted from larger documents), identify authorship links and groups of documents by the same author. All documents are single-authored, in the same language, and belong to the same genre. However, the topic or text-length of documents may vary. The number of distinct authors whose documents are included in the collection is not given.</div>
    </div>
    <div class="panel panel-default">
      <div class="panel-heading">Training Phase</div>
      <div class="panel-body"><p>To develop your software, we provide you with a training corpus that comprises a set of author clustering problems in <strong>3 languages</strong> (English, Dutch, and Greek) and <strong>2 genres</strong> (newspaper articles and reviews). Each problem consists of a set of documents in the same language and genre. However, their topic may differ and the document lengths vary from a few hundred to a few thousand words.</p>
	  <p>The documents of each problem are located in a separate folder. The file <code>info.json</code> describes all required information for each clustering problem. In more detail, the language (either <code>"en"</code>, <code>"nl"</code>, or <code>"gr"</code> for English, Dutch and Greek, respectively), genre (either <code>"articles"</code> or <code>"reviews"</code>), and the folder of each problem (relative path).</p>
	<pre class="prettyprint lang-py" style="overflow-x:auto">
[
   {"language": "en", "genre": "articles", "folder": "problem001"},
   ...
]
	</pre>
	<p>The ground truth data of the training corpus consists of two files for each clustering problem: <code>clustering.json</code> and <code>ranking.json</code> similar to the files described in the <strong>Output</strong> section (see details below). All ground truth files are located in the <code>truth</code> folder.</p>
	<p><a class="btn btn-default" target="_blank" href="http://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-17/pan17-data/pan17-author-clustering-training-dataset-2017-02-15.zip">Download corpus</a> (Updated: Feb 17, 2017)</p>
</div>
    </div>
    <div class="panel panel-default">
      <div class="panel-heading">Evaluation Phase</div>
      <div class="panel-body">Once you finished tuning your approach to achieve satisfying performance on the training corpus, your software will be tested on the evaluation corpus. During the competition, the evaluation corpus will not be released publicly. Instead, we ask you to <strong>submit your software</strong> for evaluation at our site as described below.
<br>After the competition, the evaluation corpus will become available including ground truth data. This way, you have all the necessities to evaluate your approach on your own, yet being comparable to those who took part in the competition.
	<p><a class="btn btn-default" target="_blank" href="http://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-17/pan17-data/pan17-author-clustering-test-dataset-2017-03-14.zip">Download evaluation corpus</a></p>
	    </div></div>
    <div class="panel panel-default">
      <div class="panel-heading">Output</div>
      <div class="panel-body">
<p>Your system should produce <strong>two output files</strong> in <a href="http://www.json.org/">JSON</a>:</p>
<ul>
	<li>One output file including complete information about the detected clusters named <code>clustering.json</code>. Each cluster should contain all documents found in the collection by a specific author. A JSON file of the following format should be produced (a list of clusters, each cluster is a list of documents):<li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[
	[
		{"document":  "filename1"},
		{"document":  "filename2"},
	…
	],
…
]
</pre>

<p>The clusters should be non-overlapping, thus each filename should belong to exactly one cluster.</p>

<li>One output file named <code>ranking.json</code> including a list of document pairs ranked according to a real-valued score in [0,1], where higher values denote more confidence that the pair of documents are by the same author. A JSON file of the following format should be produced (a list of document pairs and a real-valued number):</li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[
	{"document1": "filename1",
	 "document2": "filename2",
	 "score": real-valued-number},
	…
]
</pre>
<p>The order of documents within a pair is not important (e.g. "document1": "filename1", "document2": "filename2" is the same with "document2": "filename1",
 "document1": "filename2"). In case the same pair is reported more than once the first occurrence will be taken into account.</p>
</ul>

<p>An <strong>illustrating example</strong> follows. Let’s assume that a document collection of 6 files is given: file1.txt, file2.txt, file3.txt, file4.txt, file5.txt, and file6.txt. There are 3 clusters: (i) file1.txt, file3.txt, and file4.txt are by the same author, (ii) file5.txt and file6.txt are by another author and (iii) file2.txt is by yet another author. </p>
<ul>
<li>The output file in JSON for the complete author clustering task should be:</li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[   [	{"document": "file1.txt"},
		{"document": "file3.txt"},
		{"document": "file4.txt"}	],
	[
		{"document": "file5.txt"},
		{"document": "file6.txt"}	],
	[
		{"document": "file2.txt"}	]
]
</pre>
<li>An example of the output file for authorship-link ranking could be:</li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[	{"document1": "file1.txt",
	 "document2": "file4.txt",
	 "score": 0.95},

	{"document1": "file3.txt",
	 "document2": "file4.txt",
	 "score": 0.75},

	{"document1": "file5.txt",
	 "document2": "file6.txt",
	 "score": 0.66},

	{"document1": "file1.txt",
	 "document2": "file3.txt",
	 "score": 0.63}
]
</pre>
</ul>
</div></div>
    <div class="panel panel-default">
      <div class="panel-heading">Performance Measures</div>
      <div class="panel-body">
      	<ul>
      		<li>The clustering output will be evaluated according to <strong>BCubed F-score</strong> (<a href="http://nlp.uned.es/docs/amigo2007a.pdf">Amigo et al. 2007</a>)</li>
      		<li>The ranking of authorship links will be evaluated according to <strong>Mean Average Precision</strong> (<a href="http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html">Manning et al. 2008</a>)</li>
 	</ul>
	      <p>For your convenience, we provide an evaluator script written in Octave.</p>
              <p><a class="btn btn-default" href="../pan17-code/PAN17-author-clustering-evaluator.rar">Download script</a></p>
              <p>It takes three parameters: (-i) an input directory (the data set including a 'truth' folder), (-a) an answers directory (your software output) and (-o) an output directory where the evaluation results are written to. Of course, you are free to modify the script according to your needs.</p>
	</div></div>
	  <div class="panel panel-default">
      <div class="panel-heading">Submission</div>
      <div class="panel-body">
        <p>We ask you to prepare your software so that it can be executed via command line calls. The command shall take as input (i) an absolute path to the directory of the evaluation corpus and (ii) an absolute path to an empty output directory:</p>
	<pre class="prettyprint lang-py" style="overflow-x:auto">
> mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY
	</pre>
	<p>Within <code>EVALUATION-DIRECTORY</code> a <code>info.json</code> file and a number of folders, one for each clustering problem, will be found (similar to the training corpus as described above). For each clustering problem, a new folder should be built in <code>OUTPUT-DIRECTORY</code> using the same folder name found in <code>info.json</code> for that problem and within that folder the <code>clustering.json</code> and <code>ranking.json</code> output files should be written (similar to the <code>truth</code> folder of the training corpus).</p>
        <p>You can choose freely among the available programming languages and among the operating systems Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual machine that will be made accessible to you after registration. You will be able to reach the virtual machine via ssh and via remote desktop. More information about how to access the virtual machines can be found in the user guide below:</p>
        <p><a class="btn btn-default" href="http://www.tira.io/static/tira-vm-user-guide.pdf">PAN Virtual Machine User Guide »</a></p>
        <p>Once deployed in your virtual machine, we ask you to access TIRA at <a href="http://www.tira.io">www.tira.io</a>, where you can self-evaluate your software on the test data.</p>
        <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to grant us usage rights only for the purpose of the PAN competition. We agree not to share your software with a third party or use it for other purposes than the PAN competition.</p>
      </div>
    </div>
    
-->
    <div class="panel panel-default">
      <div class="panel-heading">Related Work</div>
      <div class="panel-body">
        <p>We refer you to:</p>
        <ul>	<li>
			<a href="http://pan.webis.de/clef12/pan12-web/author-identification.html">Author identification task at PAN@CLEF'12</a> (closed-set authorship attribution) </li>
		<li>
			<a href="http://pan.webis.de/clef11/pan11-web/author-identification.html">Author identification task at PAN@CLEF'11</a> (closed-set authorship attribution)</li>
	<li>
        Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3, March 2008.
        </li><li>
        Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
        Authorship Attribution</a>. Journal of the American Society for Information Science and Technology, Volume 60, Issue 1, pages 9-26, January 2009.
        </li><li>
        Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A Survey of Modern Authorship Attribution Methods</a>.
        Journal of the American Society for Information Science and Technology, Volume 60, Issue 3, pages 538-556, March 2009.
        </li></ul>
      </div>
    </div>

    <div id="task-committee-attribution" class="row">
      <div class="col-xs-12">
        <h2 class="page-header">Task Chair</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.mike-kestemont.org/" target="_blank"><img src="../pan18-figures/mike.jpg" class="img-rounded" alt="Mike Kestemont"></a>
          <p style="white-space:nowrap"><a href="http://www.mike-kestemont.org/" target="_blank">Mike Kestemont</a></p>
          <p style="font-size:10pt">University of Antwerp</p>
        </div>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-12">
        <h2>Task Committee</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.icsd.aegean.gr/lecturers/stamatatos/" target="_blank"><img src="../pan18-figures/stathis.jpg" class="img-rounded" alt="Efstathios Stamatatos"></a>
          <p><a href="http://www.icsd.aegean.gr/lecturers/stamatatos/" target="_blank">Efstathios Stamatatos</a></p>
          <p style="font-size:10pt">University of the Aegean</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.clips.ua.ac.be/~walter/" target="_blank"><img src="../pan18-figures/walter.jpg" class="img-rounded" alt="Walter Daelemans"></a>
          <p style="white-space:nowrap"><a href="http://www.clips.ua.ac.be/~walter/" target="_blank">Walter Daelemans</a></p>
          <p style="font-size:10pt">University of Antwerp</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.uni-weimar.de/medien/webis/people" target="_blank"><img src="../pan18-figures/martin.jpg" class="img-rounded" alt="Martin Potthast"></a>
          <p style="white-space:nowrap"><a href="http://www.uni-weimar.de/medien/webis/people" target="_blank">Martin Potthast</a></p>
          <p style="font-size:10pt">Bauhaus-Universit&auml;t Weimar</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.webis.de" target="_blank"><img src="../pan18-figures/benno.jpg" class="img-rounded" alt="Benno Stein"></a>
          <p style="white-space:nowrap"><a href="http://www.webis.de" target="_blank">Benno Stein</a></p>
          <p style="font-size:10pt">Bauhaus-Universit&auml;t Weimar</p>
        </div>
      </div>
   </div>
</div>

  <div class="col-sm-6">
    <h2 id="style-breach-detection">Style Change Detection</h2>
<!--
      <p>While many approaches target the problem of identifying authors of whole documents, research on investigating multi-authored documents is sparse. To narrow the gap, the <a href="../../clef16/pan16-web/author-identification.html">author diarization</a> task of the PAN-2016 edition already focused on collaboratively written documents, attempting to cluster text by authors within documents. This year we modify the problem by asking participants to detect style breaches within documents, i.e., to locate borders where authorships change.</p>
      <p>The problem is therefore related to the <strong>text segmentation</strong> problem, with the difference that the latter usually focus on detecting switches of topics or <i>stories</i>. In contrast to that, this task aims to find borders based on the writing style, disregarding the specific content. As the goal is to only find borders, it is irrelevant to identify or cluster authors of segments. A simple example consisting of four breaches of style (switches / borders) is illustrated below:</p>
      <p align="center"><img src="../pan18-figures/style-breach-sample.png"/></p>

      <div class="panel panel-default">
          <div class="panel-heading">Task</div>
          <div class="panel-body">
              <p>Given a document, determine whether it is multi-authored, and if yes, find the borders where authors switch.</p>
              <p>All documents are provided in English and may contain zero up to arbitrarily many switches (style breaches). Thereby switches of authorships may only occur at the end of sentences, i.e., not within.</p>
          </div>
      </div>

      <div class="panel panel-default">
          <div class="panel-heading">Training Phase</div>
          <div class="panel-body">
              <p>To develop your algorithms, a training data set including corresponding solutions is provided.</p>
              <p><a class="btn btn-default" target="_blank" href="http://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-17/pan17-data/pan17-style-breach-detection-training-dataset-2017-02-15.zip">Download corpus</a> (Updated: Feb 15, 2017)
              <br/><p>For each problem instance X, two files are provided:</p>
              <ul>
                  <li><code>problem-X.txt</code> contains the actual text</li>
                  <li><code>problem-X.truth</code> contains the ground truth, i.e., the correct solution in <a href="http://www.json.org/">JSON</a> format:
<pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "borders": [
        character_position_border_1,
        character_position_border_2,
        …
    ]
}
</pre>

                      <p>
                          To identify a border, the absolute character position <strong>of the first non-whitespace character of the new segment</strong> is used. The document starts at character position 0. An example that could match the borders of the image above could look as follows:</p>
                      <pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "borders": [1709, 3119, 3956, 5671]
}
</pre>

                      <p>An empty array indicates that the document is single-authored, i.e., contains no style switches:</p>
<pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "borders": []
}
</pre>
                  </li>
              </ul>
          </div>
      </div>

      <div class="panel panel-default">
          <div class="panel-heading">Evaluation Phase</div>
          <div class="panel-body">Once you finished tuning your approach to achieve satisfying performance on the training corpus, your software will be tested on the evaluation corpus. During the competition, the evaluation corpus will not be released publicly. Instead, we ask you to <strong>submit your software</strong> for evaluation at our site as described below.
              <br>After the competition, the evaluation corpus will become available including ground truth data. This way, you have all the necessities to evaluate your approach on your own, yet being comparable to those who took part in the competition.
          </div></div>


      <div class="panel panel-default">
          <div class="panel-heading">Output</div>
          <div class="panel-body">
             <p>
                 In general, the data structure during the evaluation phase will be similar to that in the training phase, with the exception that the ground truth files are missing. Thus, for each given problem <code>problem-X.txt</code> your software should output the missing solution file <code>problem-X.truth</code>. The output syntax should thereby be exactly like it is described in the training phase section.
             </p>
          </div>
      </div>


      <div class="panel panel-default">
          <div class="panel-heading">Performance Measures</div>
          <div class="panel-body">
              <p>To evaluate the predicted style breaches, two metrics will be used:
                  <ul>
                    <li>the <strong>WindowDiff</strong> metric (<a href="http://people.ischool.berkeley.edu/~hearst/papers/pevzner-01.pdf">Pevzner, Hearst, 2002</a>) was proposed for general text segmentation evaluation and is still the de facto standard for such problems. It gives an error rate (between 0 to 1, 0 indicating a perfect prediction) for predicting borders by penalizing near-misses less than other/complete misses or extra borders.</li>
              <li>a more recent adaption of the WindowDiff metric is the <strong>WinPR</strong> metric (<a href="http://www.aclweb.org/anthology/N12-1038.pdf">Scaiano, Inkpen, 2012</a>). It enhances it by computing the common information retrieval measures precision (WinP) and recall (WinR) and thus allows to give a more detailed, qualitative statement about the prediction. For the final ranking of all participating teams, the F-score of WinPR will be used.</li>
          </ul>

              <p>Note that while both metrics will be computed on a word-level, you still have to provide your solutions on a character-level (delegating the tokenization to the evaluator).</p>
              <p>For your convenience, we provide the evaluator script written in Python.</p>
              <p><a class="btn btn-default" href="../pan17-code/pan17_stylebreach_evaluator.zip">Download script</a></p>
              <p>It takes three parameters: an input directory (the data set), an inputRun directory (your computed breaches) and an output directory where the results file is written to. Of course, you are free to modify the script according to your needs.</p>

          </div>
      </div>

      <div class="panel panel-default">
          <div class="panel-heading">Submission</div>
          <div class="panel-body">
              <p>We ask you to prepare your software so that it can be executed via command line calls. The command shall take as input (i) an absolute path to the directory of the evaluation corpus and (ii) an absolute path to an empty output directory:</p>
<pre class="prettyprint lang-py" style="overflow-x:auto">
> mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY
</pre>
              <p>Within <code>EVALUATION-DIRECTORY</code>, you will find a list of problem instances, i.e., <code>[filename].txt</code> files.
              For each problem instance you should produce the solution file <code>[filename].truth</code> in the <code>OUTPUT-DIRECTORY</code> For instance, you read <code>EVALUATION-DIRECTORY/problem-12.txt</code>, process it and write your results to <code>OUTPUT-DIRECTORY/problem-12.truth</code>.</p>

              <p>You can choose freely among the available programming languages and among the operating systems Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual machine that will be made accessible to you after registration. You will be able to reach the virtual machine via ssh and via remote desktop. More information about how to access the virtual machines can be found in the user guide below:</p>
              <p><a class="btn btn-default" href="pan15-virtual-machine-user-guide.pdf">PAN Virtual Machine User Guide »</a></p>
              <p>Once deployed in your virtual machine, we ask you to access TIRA at <a href="http://www.tira.io">www.tira.io</a>, where you can self-evaluate your software on the test data.</p>
              <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to grant us usage rights only for the purpose of the PAN competition. We agree not to share your software with a third party or use it for other purposes than the PAN competition.</p>
          </div>
      </div>


      <div class="panel panel-default">
          <div class="panel-heading">Related Work</div>
          <div class="panel-body">
              <p>We refer you to:</p>
              <ul><li>
                  <a href="../../clef16/pan16-web/proceedings.html">PAN@CLEF'16</a> (<i>Clustering by Authorship Within and Across Documents</i> and <i>Author Diarization</i> section)
              </li>
                  <li>Marti A. Hearst. <a href="http://anthology.aclweb.org/J/J97/J97-1003.pdf">TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages.</a>. In Computational Linguistics, Volume 23, Issue 1, pages 33-64, 1997.</li>
                  <li>Benno Stein, Nedim Lipka and Peter Prettenhofer. <a href="http://www.uni-weimar.de/medien/webis/publications/papers/stein_2011a.pdf">Intrinsic Plagiarism Analysis</a>. In Language Resources and Evaluation, Volume 45, Issue 1, pages 63–82, 2011.</li>
                  <li>
                      Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3, March 2008.
                  </li><li>
                      Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A Survey of Modern Authorship Attribution Methods</a>.
                      Journal of the American Society for Information Science and Technology, Volume 60, Issue 3, pages 538-556, March 2009.
                  </li></ul>
          </div>
      </div>
-->



      <div id="task-committee-stylebreachdetection" class="row">
      <div class="col-xs-12">
        <h2 class="page-header">Task Chair</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://dbis-informatik.uibk.ac.at/1-1-Home.html" target="_blank"><img src="../pan18-figures/michael.png" class="img-rounded" alt="Michael Tschuggnall"></a>
          <p><a href="http://dbis-informatik.uibk.ac.at/1-1-Home.html" target="_blank">Michael Tschuggnall</a></p>
          <p style="font-size:10pt">University of Innsbruck</p>
        </div>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-12">
        <h2>Task Committee</h2>
      </div>
    </div>
    <div class="row">
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://dbis-informatik.uibk.ac.at/1-1-Home.html" target="_blank"><img src="../pan18-figures/guenther.jpg" class="img-rounded" alt="G&uuml;nther Specht"></a>
          <p><a href="http://dbis-informatik.uibk.ac.at/1-1-Home.html" target="_blank">G&uuml;nther Specht</a></p>
          <p style="font-size:10pt">University of Innsbruck</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.uni-weimar.de/medien/webis/people" target="_blank"><img src="../pan18-figures/martin.jpg" class="img-rounded" alt="Martin Potthast"></a>
          <p style="white-space:nowrap"><a href="http://www.uni-weimar.de/medien/webis/people" target="_blank">Martin Potthast</a></p>
          <p style="font-size:10pt">Bauhaus-Universit&auml;t Weimar</p>
        </div>
      </div>
      <div class="col-xs-6">
        <div class="thumbnail" style="text-align:center;">
          <a href="http://www.webis.de" target="_blank"><img src="../pan18-figures/benno.jpg" class="img-rounded" alt="Benno Stein"></a>
          <p style="white-space:nowrap"><a href="http://www.webis.de" target="_blank">Benno Stein</a></p>
          <p style="font-size:10pt">Bauhaus-Universit&auml;t Weimar</p>
        </div>
      </div>
    </div>
  </div>
</div>

<footer>
  <p class="pull-right">© pan.webis.de</p>
</footer>

</div> <!-- /container -->

<script src="../../js/jquery.js"></script>
<script src="../../js/bootstrap.min.js"></script>
<script src="../../js/prettify.js"></script>
<script>
  !function ($) {
    $(function(){
      window.prettyPrint && prettyPrint()   
    })
  }(window.jQuery)
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-70770005-1', 'auto');
  ga('send', 'pageview');

</script>

</body>
</html>

