---
layout: default
nav_active: tasks
title: PAN at CLEF 2011 - Author Identification
description: PAN at CLEF 2011 - Author Identification
---

<main>
    <header class="uk-section uk-section-muted">

        <nav class="uk-container">
            <div class="uk-align-right uk-visible@m uk-text-muted">
                <a class="uk-button" href="#" data-uk-icon="chevron-left" disabled></a>
                CLEF 2011
                <a class="uk-button uk-padding-remove-right"
                   href="{{ 'clef12/pan12-web/author-identification.html' | relative_url }}"
                   data-uk-icon="chevron-right"></a>
            </div>

            <ul class="uk-text-muted uk-tab uk-margin-remove-top">
                <li><a href="{{ '/clef11/pan11-web/index.html' | relative_url }}">Overview</a></li>
                <li><a href="{{ '/clef11/pan11-web/keynotes.html' | relative_url }}">Keynotes</a></li>
                <li><a href="{{ '/clef11/pan11-web/program.html' | relative_url }}">Program</a></li>
                <li class="uk-active">
                    <a href="#">Tasks <span class="uk-margin-small-left"
                                            data-uk-icon="icon: chevron-down"></span></a>
                    <div class="uk-dropdown" data-uk-dropdown="mode: click">
                        <ul class="uk-nav uk-dropdown-nav">
                            <li class="uk-active"><a
                                    href="{{ '/clef11/pan11-web/author-identification.html' | relative_url }}">Author
                                Identification</a></li>
                            <li><a href="{{ '/clef11/pan11-web/wikipedia-vandalism-detection.html' | relative_url }}">
                                Wikipedia Vandalism Detection</a></li>
                            <li><a href="{{ '/clef11/pan11-web/plagiarism-detection.html' | relative_url }}">Plagiarism
                                Detection</a></li>
                        </ul>
                    </div>
                </li>
                <li><a href="{{ '/clef11/pan11-web/submission.html' | relative_url }}">Submission</a></li>
                <li><a href="{{ '/clef11/pan11-web/proceedings.html' | relative_url }}">Proceedings</a></li>
            </ul>
        </nav>

        <div class="uk-container uk-margin-small">
            {% include current-register-quicklink.html year=2011 %}
            <h1 class="uk-margin-small">Author Identification</h1>
            <div class="page-header-meta">PAN at CLEF 2011</div>
        </div>

        <div class="uk-container uk-margin-medium">
            <p class="uk-text-lead">Within author identification, we address both authorship attribution and authorship
                verification.</p>
        </div>
    </header>

    <section>
        <div class="uk-section">
            <div class="uk-container">
                <div class="uk-column-1-2@l uk-text-justify">
                    <h3>Task</h3>
                    <ul>
                        <li>
                            Given texts of uncertain authorship and texts from a set of candidate authors, the task is
                            to map the uncertain texts onto their true authors among the candidates.
                        </li>
                        <li>
                            Given a text of uncertain authorship and text from a specific author, the task is to
                            determine whether the given text has been written by that author.
                        </li>
                    </ul>


                    <h3>Training Corpus</h3>
                    <p>To develop your software, we provide you with a training corpus that comprises several different
                        common attribution and verification scenarios. There are five training collections consisting of
                        real-world texts (for authorship attribution), and three each with a single author (for
                        authorship verification).</p>
                    <p>
                        <a class="uk-button uk-button-primary"
                           href="https://www.uni-weimar.de/medien/webis/events/pan-11/pan11-papers-final/pan11-author-identification/argamon11-overview.pdf#page=2">Learn
                            more »</a>
                        <a class="uk-button uk-button-primary" target="_blank"
                           href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-11/pan11-data/pan11-author-identification-training-corpus-2011-04-08.zip">Download
                            corpus</a>
                    </p>


                    <h3>Output</h3>
                    Classification results are to be formatted in an XML document similar to those found in the
                    evaluation corpora, just without the <code>&lt;body&gt;</code> element.


                    <h3>Performance Measures</h3>
                    The performance of your authorship attribution will be judged by average precision, recall, and F1
                    over all authors in the given training set.


                    <h3>Test Corpus</h3>
                    <p>Once you finished tuning your approach to achieve satisfying performance on the training corpus,
                        you should run your software on the test corpus.</p>
                    <p>During the competition, the test corpus will not be released publicly. Instead, we ask you to
                        submit your software for evaluation at our site as described below.</p>
                    <p>After the competition, the test corpus is available including ground truth data. This way, you
                        have all the necessities to evaluate your approach on your own, yet being comparable to those
                        who took part in the competition.</p>
                    <p>
                        <a class="uk-button uk-button-primary" target="_blank"
                           href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-11/pan11-data/pan11-author-identification-test-corpus-2011-05-23.zip">Download
                            corpus</a>
                    </p>


                    <h3>Submission</h3>
                    <p>To submit your test run for evaluation, we ask you to send a Zip archive containing the output of
                        your software when run on the test corpus to <a href="mailto:pan@webis.de">pan@webis.de</a>.</p>
                    <p>Should the Zip archive be too large to be sent via mail, please upload it to a file hoster of
                        your choosing and share a download link with us.</p>


                    <h3>Results</h3>
                    <p>The results of the evaluation have been compiled into <a
                            href="pan11-author-identification-evaluation-results.xlsx">this Excel sheet</a>s. The table
                        shows micro-averaged and macro-averaged precision, recall, and F values for all runs submitted,
                        dependent on the authorship sub-task. Moreover, the respective rankings obtained from these
                        values are shown. The last column shows an overall ranking based on the sum of all ranks.</p>
                    <p>The authorship attribution approach of Ludovic Tanguy (University of Toulouse & CNRS, France) was
                        highly ranked across all the attribution tasks. It was beaten only once significantly by the
                        approach of Ioannis Kourtis (University of the Aegean, Greece) on the Large task. The approach
                        of Mario Zechner, (Know-Center, Austria) achieved very high precision on the small attribution
                        tasks, but paid for it in recall.</p>
                    <p>Regarding the authorship verification tasks, Tim Snider (Porfiau, Canada) achieved the best
                        precision performance overall, though not the highest recall. It must be mentioned that
                        authorship verification is more difficult to be accomplished than authorship attribution.
                        Moreover, the distinction between macro- and micro-averaged performance measuring is meaningless
                        in this case, since there is only one author of interest which is why just one set of results is
                        reported.</p>
                    <p>A more detailed analysis of the detection performances can be found in the overview paper
                        accompanying this task.</p>
                    <p>
                        <a class="uk-button uk-button-primary" href="pan11-author-identification-evaluation-results.xlsx">Complete
                            performances (Excel)</a>
                        <a class="uk-button uk-button-primary"
                           href="https://www.uni-weimar.de/medien/webis/events/pan-11/pan11-papers-final/pan11-author-identification/argamon11-overview.pdf#page=6">Learn
                            more »</a>
                    </p>

                    <h3>Related Work</h3>
                    <p>For an overview of approaches to automated authorship attribution, we refer you to recent survey
                        papers in the area:</p>
                    <ul>
                        <li>
                            Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451" target="_blank">Authorship
                            Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                            December 2006.
                        </li>
                        <li>
                            Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                                href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full" target="_blank">Computational
                            Methods in Authorship Attribution</a>. Journal of the American Society for Information
                            Science and Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                        </li>
                        <li>
                            Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full"
                                                      target="_blank">A Survey of Modern Authorship Attribution
                            Methods</a>. Journal of the American Society for Information Science and Technology, Volume
                            60, Issue 3, pages 538-556, March 2009.
                        </li>
                    </ul>

                </div>
            </div>
        </div>

        <div class="uk-section uk-section-muted">
            <div class="uk-container">

                <h2>Task Chair</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/juola.html %}
                </div>

                <h2>Task Committee</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/stamatatos.html %}
                    {% include people-cards/argamon.html %}
                    {% include people-cards/koppel.html %}
                </div>
            </div>
        </div>
    </section>
</main>
