---
layout: default
nav_active: tasks
title: PAN @ CLEF 2017 - Author Identification
description: PAN @ CLEF 2017 - Author Identification
---

<main>
    <header class="uk-section uk-section-muted">

        <nav class="uk-container">
            <div class="uk-align-right uk-visible@m uk-text-muted">
                <a class="uk-button" href="{{ 'clef16/pan16-web/author-identification.html' | relative_url }}"
                   data-uk-icon="chevron-left"></a>
                CLEF 2017
                <a class="uk-button uk-padding-remove-right"
                   href="{{ 'clef18/pan18-web/author-identification.html' | relative_url }}"
                   data-uk-icon="chevron-right"></a>
            </div>

            <ul class="uk-text-muted uk-tab uk-margin-remove-top">
                <li><a href="{{ '/clef17/pan17-web/index.html' | relative_url }}">Overview</a></li>
                <li><a href="{{ '/clef17/pan17-web/submission.html' | relative_url }}">Submission</a></li>
                <li class="uk-active">
                    <a href="#">Tasks <span class="uk-margin-small-left"
                                            data-uk-icon="icon: chevron-down"></span></a>
                    <div class="uk-dropdown" data-uk-dropdown="mode: click">
                        <ul class="uk-nav uk-dropdown-nav">
                            <li class="uk-active"><a
                                    href="{{ '/clef17/pan17-web/author-identification.html' | relative_url }}">Author
                                Identification</a></li>
                            <li><a href="{{ '/clef17/pan17-web/author-profiling.html' | relative_url }}">Author
                                Profiling</a></li>
                            <li><a href="{{ '/clef17/pan17-web/author-obfuscation.html' | relative_url }}">Author
                                Obfuscation</a></li>
                        </ul>
                    </div>
                </li>
                <li><a href="{{ '/clef17/pan17-web/program.html' | relative_url }}">Program</a></li>
                <li><a href="{{ '/clef17/pan17-web/proceedings.html' | relative_url }}">Proceedings</a></li>
            </ul>
        </nav>

        <div class="uk-container uk-margin-small">
            {% include current-register-quicklink.html year=2017 %}
            <h1 class="uk-margin-small">Author Identification</h1>
            <div class="page-header-meta">PAN @ CLEF 2017</div>
        </div>

        <div class="uk-container uk-margin-medium">
            <p class="uk-text-lead">This task is divided into <a href="#author-clustering">author clustering</a> and
                <a href="#style-breach-detection">style breach detection</a>. You can choose to solve one or both of
                them.</p>
        </div>
    </header>

    <section>
        <div class="uk-section">
            <div class="uk-container">
                <div class="uk-column-1-2@l uk-text-justify">
                    <h2 id="author-clustering">Author Clustering</h2>
                    <p>Authorship attribution is an important problem in information retrieval and computational
                        linguistics but also in applied areas such as law and journalism where knowing the author of a
                        document (such as a ransom note) may be able to save lives. The most common framework for
                        testing candidate algorithms is the closed-set attribution task: given known sample documents
                        from a small, finite set of candidate authors, which wrote a questioned document of unknown
                        authorship? It has been commented, however, that this may be an unreasonably easy task. A more
                        demanding task is author clustering where given a document collection the task is to group
                        documents written by the same author so that each cluster corresponds to a different author.
                        This task can also be viewed as establishing authorship links between documents. </p>
                    <p>Note that the task of authorship verification studied in detail in previous editions of PAN
                        (2013-2015) is strongly associated with author clustering since any clustering problem can be
                        decomposed into a series of author verification problems. We encourage participants to attempt
                        to modify authorship verification approaches to deal with the author clustering task.</p>
                    <p>PAN-2016 first studied the author clustering task focusing on relatively long documents like
                        articles and reviews. In this edition of PAN, we focus on short documents of paragraph length.
                        The aim is to cluster paragraphs that may be extracted from the same document or different
                        documents and are by the same author. Such a task is closely related to author diarization and
                        intrinsic plagiarism detection. </p>
                    <p>Similar to PAN-2016 edition of the task, two application scenarios are examined: </p>
                    <ul>
                        <li><strong>Complete author clustering</strong>: This scenario requires a detailed analysis
                            where the number (k) of different authors found in the collection should be identified and
                            each document should be assigned to exactly one of the k clusters (each cluster corresponds
                            to a different author). In the following illustrating example, 4 different authors are found
                            and the colour of each document indicates its author.
                        </li>
                        <p align="center"><img
                                src="https://cloud.githubusercontent.com/assets/15824066/12065874/81d95440-afe7-11e5-828b-54e293540823.png"/>
                        </p>
                        <li><strong>Authorship-link ranking</strong>: This scenario views the exploration of the given
                            document collection as a retrieval task. It aims at establishing authorship links between
                            documents and provides a list of document pairs ranked according to a confidence score (the
                            score shows how likely it is the document pair to be by the same author). In the following
                            example, 4 document pairs with similar authorship are found and then these authorship-links
                            are sorted according to their similarity.
                        </li>
                        <p align="center"><img
                                src="https://cloud.githubusercontent.com/assets/15824066/12065971/f2a087e2-afe8-11e5-9ef4-6df4e5aff9ae.png"/>
                        </p>
                    </ul>


                    <h3>Task</h3>
                    <p>Given a collection of (up to 50) short documents (paragraphs extracted from larger documents),
                        identify authorship links and groups of documents by the same author. All documents are
                        single-authored, in the same language, and belong to the same genre. However, the topic or
                        text-length of documents may vary. The number of distinct authors whose documents are included
                        in
                        the collection is not given.</p>

                    <h3>Training Phase</h3>
                    <p>To develop your software, we provide you with a training corpus that comprises a set of author
                        clustering problems in <strong>3 languages</strong> (English, Dutch, and Greek) and <strong>2
                            genres</strong> (newspaper articles and reviews). Each problem consists of a set of
                        documents in the same language and genre. However, their topic may differ and the document
                        lengths vary from a few hundred to a few thousand words.</p>
                    <p>The documents of each problem are located in a separate folder. The file <code>info.json</code>
                        describes all required information for each clustering problem. In more detail, the language
                        (either <code>"en"</code>, <code>"nl"</code>, or <code>"gr"</code> for English, Dutch and Greek,
                        respectively), genre (either <code>"articles"</code> or <code>"reviews"</code>), and the folder
                        of each problem (relative path).</p>
                    <pre class="prettyprint lang-py" style="overflow-x:auto">
[
   {"language": "en", "genre": "articles", "folder": "problem001"},
   ...
]
	</pre>
                    <p>The ground truth data of the training corpus consists of two files for each clustering problem:
                        <code>clustering.json</code> and <code>ranking.json</code> similar to the files described in the
                        <strong>Output</strong> section (see details below). All ground truth files are located in the
                        <code>truth</code> folder.</p>
                    <p><a class="uk-button uk-button-primary" target="_blank"
                          href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-17/pan17-data/pan17-author-clustering-training-dataset-2017-02-15.zip">Download
                        corpus</a> (Updated: Feb 17, 2017)</p>


                    <h3>Evaluation Phase</h3>
                    <p>Once you finished tuning your approach to achieve satisfying performance on the training corpus,
                        your software will be tested on the evaluation corpus. During the competition, the evaluation
                        corpus
                        will not be released publicly. Instead, we ask you to <strong>submit your software</strong> for
                        evaluation at our site as described below.</p>
                    <p>After the competition, the evaluation corpus will become available including ground truth data.
                        This way, you have all the necessities to evaluate your approach on your own, yet being
                        comparable
                        to those who took part in the competition.</p>
                    <p><a class="uk-button uk-button-primary" target="_blank"
                          href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-17/pan17-data/pan17-author-clustering-test-dataset-2017-03-14.zip">Download
                        evaluation corpus</a></p>


                    <h3>Output</h3>
                    <p>Your system should produce <strong>two output files</strong> in <a href="http://www.json.org/">JSON</a>:
                    </p>
                    <ul>
                        <li>One output file including complete information about the detected clusters named <code>clustering.json</code>.
                            Each cluster should contain all documents found in the collection by a specific author. A
                            JSON file of the following format should be produced (a list of clusters, each cluster is a
                            list of documents):
                        <li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[
	[
		{"document":  "filename1"},
		{"document":  "filename2"},
	…
	],
…
]
</pre>

                            <p>The clusters should be non-overlapping, thus each filename should belong to exactly one
                                cluster.</p>

                        <li>One output file named <code>ranking.json</code> including a list of document pairs ranked
                            according to a real-valued score in [0,1], where higher values denote more confidence that
                            the pair of documents are by the same author. A JSON file of the following format should be
                            produced (a list of document pairs and a real-valued number):
                        </li>
                        <pre class="prettyprint lang-py" style="overflow-x:auto">
[
	{"document1": "filename1",
	 "document2": "filename2",
	 "score": real-valued-number},
	…
]
</pre>
                        <p>The order of documents within a pair is not important (e.g. "document1": "filename1",
                            "document2": "filename2" is the same with "document2": "filename1",
                            "document1": "filename2"). In case the same pair is reported more than once the first
                            occurrence will be taken into account.</p>
                    </ul>

                    <p>An <strong>illustrating example</strong> follows. Let’s assume that a document collection of 6
                        files is given: file1.txt, file2.txt, file3.txt, file4.txt, file5.txt, and file6.txt. There are
                        3 clusters: (i) file1.txt, file3.txt, and file4.txt are by the same author, (ii) file5.txt and
                        file6.txt are by another author and (iii) file2.txt is by yet another author. </p>
                    <ul>
                        <li>The output file in JSON for the complete author clustering task should be:</li>
                        <pre class="prettyprint lang-py" style="overflow-x:auto">
[   [	{"document": "file1.txt"},
		{"document": "file3.txt"},
		{"document": "file4.txt"}	],
	[
		{"document": "file5.txt"},
		{"document": "file6.txt"}	],
	[
		{"document": "file2.txt"}	]
]
</pre>
                        <li>An example of the output file for authorship-link ranking could be:</li>
                        <pre class="prettyprint lang-py" style="overflow-x:auto">
[	{"document1": "file1.txt",
	 "document2": "file4.txt",
	 "score": 0.95},

	{"document1": "file3.txt",
	 "document2": "file4.txt",
	 "score": 0.75},

	{"document1": "file5.txt",
	 "document2": "file6.txt",
	 "score": 0.66},

	{"document1": "file1.txt",
	 "document2": "file3.txt",
	 "score": 0.63}
]
</pre>
                    </ul>

                    <h3>Performance Measures</h3>
                    <ul>
                        <li>The clustering output will be evaluated according to <strong>BCubed F-score</strong> (<a
                                href="http://nlp.uned.es/docs/amigo2007a.pdf">Amigo et al. 2007</a>)
                        </li>
                        <li>The ranking of authorship links will be evaluated according to <strong>Mean Average
                            Precision</strong> (<a
                                href="http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html">Manning
                            et al. 2008</a>)
                        </li>
                    </ul>
                    <p>For your convenience, we provide an evaluator script written in Octave.</p>
                    <p><a class="uk-button uk-button-primary"
                          href="{{ '/clef17/pan17-code/PAN17-author-clustering-evaluator.rar' | relative_url }}">Download
                        script</a></p>
                    <p>It takes three parameters: (-i) an input directory (the data set including a 'truth' folder),
                        (-a) an answers directory (your software output) and (-o) an output directory where the
                        evaluation results are written to. Of course, you are free to modify the script according to
                        your needs.</p>


                    <h3>Submission</h3>
                    <p>We ask you to prepare your software so that it can be executed via command line calls. The
                        command shall take as input (i) an absolute path to the directory of the evaluation corpus and
                        (ii) an absolute path to an empty output directory:</p>
                    <pre class="prettyprint lang-py" style="overflow-x:auto">
> mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY
	</pre>
                    <p>Within <code>EVALUATION-DIRECTORY</code> a <code>info.json</code> file and a number of folders,
                        one for each clustering problem, will be found (similar to the training corpus as described
                        above). For each clustering problem, a new folder should be built in
                        <code>OUTPUT-DIRECTORY</code> using the same folder name found in <code>info.json</code> for
                        that problem and within that folder the <code>clustering.json</code> and
                        <code>ranking.json</code> output files should be written (similar to the <code>truth</code>
                        folder of the training corpus).</p>
                    <p>You can choose freely among the available programming languages and among the operating systems
                        Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual machine
                        that will be made accessible to you after registration. You will be able to reach the virtual
                        machine via ssh and via remote desktop. More information about how to access the virtual
                         class="uk-list uk-list-bullet"machines can be found in the user guide below:</p>
                    <p><a class="uk-button uk-button-primary" href="http://www.tira.io/static/tira-vm-user-guide.pdf">PAN Virtual
                        Machine User Guide »</a></p>
                    <p>Once deployed in your virtual machine, we ask you to access TIRA at <a href="http://www.tira.io">www.tira.io</a>,
                        where you can self-evaluate your software on the test data.</p>
                    <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to grant
                        us usage rights only for the purpose of the PAN competition. We agree not to share your software
                        with a third party or use it for other purposes than the PAN competition.</p>


                    <h3>Related Work</h3>
                    <p>We refer you to:</p>
                    <ul class="uk-list uk-list-bullet">
                        <li>
                            <a href="../../clef16/pan16-web/proceedings.html">Author clustering task at PAN@CLEF'16</a>
                        </li>
                        <li>
                            <a href="../../clef15/pan15-web/proceedings.html">Author verification task at
                                PAN@CLEF'15</a></li>
                        <li>
                            <a href="../../clef14/pan14-web/proceedings.html">Author verification task at
                                PAN@CLEF'14</a></li>
                        <li>
                            <a href="../../clef13/pan13-web/proceedings.html">Author verification task at
                                PAN@CLEF'13</a></li>
                        <li>
                            Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                            Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                            March 2008.
                        </li>
                        <li>
                            Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                                href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
                            Authorship Attribution</a>. Journal of the American Society for Information Science and
                            Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                        </li>
                        <li>
                            Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                            Survey of Modern Authorship Attribution Methods</a>.
                            Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                            pages 538-556, March 2009.
                        </li>
                    </ul>

                </div>
            </div>
        </div>

        <div class="uk-section uk-section-muted">
            <div class="uk-container">

                <h2>Task Chair</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/stamatatos.html %}
                </div>

                <h2>Task Committee</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/daelemans.html %}
                    {% include people-cards/potthast.html %}
                    {% include people-cards/stein.html %}
                    {% include people-cards/verhoeven.html %}
                </div>
            </div>
        </div>
    </section>

    <section>
        <div class="uk-section">
            <div class="uk-container">
                <div class="uk-column-1-2@l uk-text-justify">
                    <h2 id="style-breach-detection">Style Change Detection</h2>
                    <p>While many approaches target the problem of identifying authors of whole documents, research on
                        investigating multi-authored documents is sparse. To narrow the gap, the <a
                                href="../../clef16/pan16-web/author-identification.html">author diarization</a> task of
                        the PAN-2016 edition already focused on collaboratively written documents, attempting to cluster
                        text by authors within documents. This year we modify the problem by asking participants to
                        detect style breaches within documents, i.e., to locate borders where authorships change.</p>
                    <p>The problem is therefore related to the <strong>text segmentation</strong> problem, with the
                        difference that the latter usually focus on detecting switches of topics or <i>stories</i>. In
                        contrast to that, this task aims to find borders based on the writing style, disregarding the
                        specific content. As the goal is to only find borders, it is irrelevant to identify or cluster
                        authors of segments. A simple example consisting of four breaches of style (switches / borders)
                        is illustrated below:</p>
                    <p align="center"><img src="../pan17-figures/style-breach-sample.png"/></p>

                    <h3>Task</h3>
                    <p>Given a document, determine whether it is multi-authored, and if yes, find the borders where
                        authors switch.</p>
                    <p>All documents are provided in English and may contain zero up to arbitrarily many switches (style
                        breaches). Thereby switches of authorships may only occur at the end of sentences, i.e., not
                        within.</p>


                    <h3>Training Phase</h3>
                    <p>To develop your algorithms, a training data set including corresponding solutions is
                        provided.</p>
                    <p><a class="uk-button uk-button-primary" target="_blank"
                          href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-17/pan17-data/pan17-style-breach-detection-training-dataset-2017-02-15.zip">Download
                        corpus</a> (Updated: Feb 15, 2017)
                        <br/>
                    <p>For each problem instance X, two files are provided:</p>
                    <ul>
                        <li><code>problem-X.txt</code> contains the actual text</li>
                        <li><code>problem-X.truth</code> contains the ground truth, i.e., the correct solution in <a
                                href="http://www.json.org/">JSON</a> format:
                            <pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "borders": [
        character_position_border_1,
        character_position_border_2,
        …
    ]
}
</pre>

                            <p>
                                To identify a border, the absolute character position <strong>of the first
                                non-whitespace character of the new segment</strong> is used. The document starts at
                                character position 0. An example that could match the borders of the image above could
                                look as follows:</p>
                            <pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "borders": [1709, 3119, 3956, 5671]
}
</pre>

                            <p>An empty array indicates that the document is single-authored, i.e., contains no style
                                switches:</p>
                            <pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "borders": []
}
</pre>
                        </li>
                    </ul>

                    <h3>Evaluation Phase</h3>
                    Once you finished tuning your approach to achieve satisfying performance on the training corpus,
                    your software will be tested on the evaluation corpus. During the competition, the evaluation corpus
                    will not be released publicly. Instead, we ask you to <strong>submit your software</strong> for
                    evaluation at our site as described below.
                    <br>After the competition, the evaluation corpus will become available including ground truth data.
                    This way, you have all the necessities to evaluate your approach on your own, yet being comparable
                    to those who took part in the competition.
                    <p></p>
                    <a class="uk-button uk-button-primary" target="_blank"
                       href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-17/pan17-data/pan17-style-breach-detection-test-dataset-2017-02-15.zip">Download
                        test data</a>


                    <h3>Output</h3>
                    <p>
                        In general, the data structure during the evaluation phase will be similar to that in the
                        training phase, with the exception that the ground truth files are missing. Thus, for each given
                        problem <code>problem-X.txt</code> your software should output the missing solution file <code>problem-X.truth</code>.
                        The output syntax should thereby be exactly like it is described in the training phase section.
                    </p>

                    <h3>Performance Measures</h3>
                    <p>To evaluate the predicted style breaches, two metrics will be used:
                    <ul>
                        <li>the <strong>WindowDiff</strong> metric (<a
                                href="http://people.ischool.berkeley.edu/~hearst/papers/pevzner-01.pdf">Pevzner, Hearst,
                            2002</a>) was proposed for general text segmentation evaluation and is still the de facto
                            standard for such problems. It gives an error rate (between 0 to 1, 0 indicating a perfect
                            prediction) for predicting borders by penalizing near-misses less than other/complete misses
                            or extra borders.
                        </li>
                        <li>a more recent adaption of the WindowDiff metric is the <strong>WinPR</strong> metric (<a
                                href="http://www.aclweb.org/anthology/N12-1038.pdf">Scaiano, Inkpen, 2012</a>). It
                            enhances it by computing the common information retrieval measures precision (WinP) and
                            recall (WinR) and thus allows to give a more detailed, qualitative statement about the
                            prediction. For the final ranking of all participating teams, the F-score of WinPR will be
                            used.
                        </li>
                    </ul>

                    <p>Note that while both metrics will be computed on a word-level, you still have to provide your
                        solutions on a character-level (delegating the tokenization to the evaluator).</p>
                    <p>For your convenience, we provide the evaluator script written in Python.</p>
                    <p><a class="uk-button uk-button-primary" href="{{ '/clef17/pan17-code/pan17_stylebreach_evaluator.zip' | relative_url }}">Download
                        script</a></p>
                    <p>It takes three parameters: an input directory (the data set), an inputRun directory (your
                        computed breaches) and an output directory where the results file is written to. Of course, you
                        are free to modify the script according to your needs.</p>


                    <h3>Submission</h3>
                    <p>We ask you to prepare your software so that it can be executed via command line calls. The
                        command shall take as input (i) an absolute path to the directory of the evaluation corpus and
                        (ii) an absolute path to an empty output directory:</p>
                    <pre class="prettyprint lang-py" style="overflow-x:auto">
> mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY
</pre>
                    <p>Within <code>EVALUATION-DIRECTORY</code>, you will find a list of problem instances, i.e., <code>[filename].txt</code>
                        files.
                        For each problem instance you should produce the solution file <code>[filename].truth</code> in
                        the <code>OUTPUT-DIRECTORY</code> For instance, you read <code>EVALUATION-DIRECTORY/problem-12.txt</code>,
                        process it and write your results to <code>OUTPUT-DIRECTORY/problem-12.truth</code>.</p>

                    <p>You can choose freely among the available programming languages and among the operating systems
                        Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual machine
                        that will be made accessible to you after registration. You will be able to reach the virtual
                        machine via ssh and via remote desktop. More information about how to access the virtual
                        machines can be found in the user guide below:</p>
                    <p><a class="uk-button uk-button-primary" href="pan15-virtual-machine-user-guide.pdf">PAN Virtual Machine User
                        Guide »</a></p>
                    <p>Once deployed in your virtual machine, we ask you to access TIRA at <a href="http://www.tira.io">www.tira.io</a>,
                        where you can self-evaluate your software on the test data.</p>
                    <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to grant
                        us usage rights only for the purpose of the PAN competition. We agree not to share your software
                        with a third party or use it for other purposes than the PAN competition.</p>


                    <h3>Related Work</h3>
                    <p>We refer you to:</p>
                    <ul class="uk-list uk-list-bullet">
                        <li>
                            <a href="../../clef16/pan16-web/proceedings.html">PAN@CLEF'16</a> (<i>Clustering by
                            Authorship Within and Across Documents</i> and <i>Author Diarization</i> section)
                        </li>
                        <li>Marti A. Hearst. <a href="http://anthology.aclweb.org/J/J97/J97-1003.pdf">TextTiling:
                            Segmenting Text into Multi-paragraph Subtopic Passages.</a>. In Computational Linguistics,
                            Volume 23, Issue 1, pages 33-64, 1997.
                        </li>
                        <li>Benno Stein, Nedim Lipka and Peter Prettenhofer. <a
                                href="https://www.uni-weimar.de/medien/webis/publications/papers/stein_2011a.pdf">Intrinsic
                            Plagiarism Analysis</a>. In Language Resources and Evaluation, Volume 45, Issue 1, pages
                            63–82, 2011.
                        </li>
                        <li>
                            Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                            Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                            March 2008.
                        </li>
                        <li>
                            Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                            Survey of Modern Authorship Attribution Methods</a>.
                            Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                            pages 538-556, March 2009.
                        </li>
                    </ul>
                </div>
            </div>
        </div>  <!-- section -->

        <div class="uk-section uk-section-muted">
            <div class="uk-container">

                <h2>Task Chair</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/tschuggnall.html %}
                </div>

                <h2>Task Committee</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/specht.html %}
                    {% include people-cards/potthast.html %}
                    {% include people-cards/stein.html %}
                </div>
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2017 %}
            </div>
        </div>
    </section>
</main>
