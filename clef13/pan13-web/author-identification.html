---
layout: default
nav_active: tasks
title: PAN at CLEF 2013 - Author Identification
description: PAN at CLEF 2013 - Author Identification
---

<main>
    <header class="uk-section uk-section-muted">

        <nav class="uk-container">
            <div class="uk-align-right uk-visible@m uk-text-muted">
                <a class="uk-button" href="{{ 'clef12/pan12-web/author-identification.html' | relative_url }}"
                   data-uk-icon="chevron-left"></a>
                CLEF 2013
                <a class="uk-button uk-padding-remove-right"
                   href="{{ 'clef14/pan14-web/author-identification.html' | relative_url }}"
                   data-uk-icon="chevron-right"></a>
            </div>

            <ul class="uk-text-muted uk-tab uk-margin-remove-top">
                <li><a href="{{ '/clef13/pan13-web/index.html' | relative_url }}">Overview</a></li>
                <li><a href="{{ '/clef13/pan13-web/keynotes.html' | relative_url }}">Keynotes</a></li>
                <li><a href="{{ '/clef13/pan13-web/program.html' | relative_url }}">Program</a></li>
                <li class="uk-active">
                    <a href="#">Tasks <span class="uk-margin-small-left"
                                            data-uk-icon="icon: chevron-down"></span></a>
                    <div class="uk-dropdown" data-uk-dropdown="mode: click">
                        <ul class="uk-nav uk-dropdown-nav">
                            <li class="uk-active"><a
                                    href="{{ '/clef13/pan13-web/author-identification.html' | relative_url }}">Author
                                Identification</a></li>
                            <li><a
                                    href="{{ '/clef13/pan13-web/author-profiling.html' | relative_url }}">Author
                                Profiling</a></li>
                            <li><a href="{{ '/clef13/pan13-web/plagiarism-detection.html' | relative_url }}">Plagiarism
                                Detection</a></li>
                        </ul>
                    </div>
                </li>
                <li><a href="{{ '/clef13/pan13-web/submission.html' | relative_url }}">Submission</a></li>
                <li><a href="{{ '/clef13/pan13-web/proceedings.html' | relative_url }}">Proceedings</a></li>
            </ul>
        </nav>

        <div class="uk-container uk-margin-small">
            {% include current-register-quicklink.html year=2013 %}
            <h1 class="uk-margin-small">Author Identification</h1>
            <div class="page-header-meta">PAN at CLEF 2013</div>
        </div>

        <div class="uk-container uk-margin-medium">
            <p class="uk-text-lead">This task focuses on <strong>authorship verification</strong> and methods to answer
                the question
                whether two given documents have the same author or no. This question accurately emulates the real-world
                problem that most forensic linguists face every day.</p>
        </div>
    </header>

    <section>
        <div class="uk-section">
            <div class="uk-container">
                <div class="uk-column-1-2@l uk-text-justify">
                    <h2>Author Identification</h2>
                    <p>Authorship attribution is an important problem in many areas including information retrieval and
                        computational linguistics, but also in applied areas such as law and journalism where knowing
                        the author of a document (such as a ransom note) may be able to save lives. The most common
                        framework for testing candidate algorithms is a text classification problem: given known sample
                        documents from a small, finite set of candidate authors, which if any wrote a questioned
                        document of unknown authorship? It has been commented, however, that this may be an unreasonably
                        easy task. A more demanding problem is author verification where given a set of documents by a
                        single author and a questioned document, the problem is to determine if the questioned document
                        was written by that particular author or not. This may more accurately reflect real life in the
                        experiences of professional forensic linguists, who are often called upon to answer this kind of
                        question.</p>
                    <p><strong>A note to forensic linguists:</strong> In order to bridge the gap between linguistics and
                        computer science, we strongly encourage submissions from researchers from both fields. We
                        understand that research groups with expertise in linguistics use manual or semi-automated
                        methods and, therefore, they are not able to submit their software. To enable their
                        participation, we will provide them with the opportunity to analyze the test corpus after the
                        deadline of software submission (mid-April). Their results will be ranked in a separate list
                        with respect to the performance of the software submissions and they will be entitled to
                        describe their approach in a paper.
                    </p>

                    <h3>Task</h3>
                    <p>Given a small set (no more than 10, possibly as few as one) of "known" documents by a single
                        person and a "questioned" document, the task is to determine whether the questioned document was
                        written by the same person who wrote the known document set.
                    </p>

                    <h3>Training Corpus</h3>
                    <p>To develop your software, we provide you with a training corpus that comprises a set of known
                        documents by a single person and exactly one questioned document. There are several such problem
                        instances covering English, Greek, and Spanish and a varying number of known documents (1-10 per
                        problem). All documents within a single problem instance will be in the same language and best
                        efforts are applied to assure that within-problem documents are matched for genre, register,
                        theme, and date of writing. The document lengths vary from a few hundred to a few thousand
                        words.</p>
                    <p><a class="uk-button uk-button-primary" target="_blank"
                          href="https://www.uni-weimar.de/medien/webis/events/pan-13/pan13-papers-final/pan13-authorship-verification/juola13-overview.pdf#page=3">Learn
                        more</a>
                    <p><a class="uk-button uk-button-primary" target="_blank"
                          href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-13/pan13-data/pan13-authorship-verification-training-corpus-2013-02-01.zip">Download
                        corpus</a>
                    </p>


                    <h3>Output</h3>
                    <p>Your software must take as input the absolute path to a set of problems. For each problem there
                        is a separate sub-folder within that path including the set of known documents and the single
                        unknown document of that problem. The software has to output a single text file <code>answers.txt</code>
                        with all the produced answers for the whole set of evaluation problems. Each line of this file
                        corresponds to a problem instance, it starts with the ID of the problem followed by a binary
                        (Y)ES/(N)O answer to the question "Is the unknown document written by the author of the known
                        documents?". If you do not want to provide answers for some problems, you can either replace the
                        answer character with "-" or just do not include a line for that problem to your answers. For
                        example, an <code>answers.txt</code> file may look like this:</p>
                    <pre class="prettyprint lang-py" style="overflow-x:auto">
EN01 Y
EN02 N
EN03 -
EN04 Y
EN07 Y
...
</pre>
                    <p>Optionally, you may also provide a score, a real number in the set [0,1] inclusive, where 0
                        corresponds to NO and 1 to YES. This score should be round with two decimal digits and will
                        allow a more detailed evaluation of your approach. In this case, the scores have to be placed
                        next to the binary answers. It is possible to provide scores even for problems you are not able
                        to provide binary answers. For example, an <code>answers.txt</code> file with scores may look
                        like this:</p>
                    <pre class="prettyprint lang-py" style="overflow-x:auto">
EN01 Y 0.90
EN02 N 0.25
EN03 - 0.53
EN04 Y 0.86
EN07 Y 0.74
...
</pre>
                    <p>Use a single whitespace to separate problem ID, binary answer, and score. The naming of the
                        output file is up to you. We reccomend to use the name of the participant group-run.</p>


                    <h3>Performance Measures</h3>
                    <p>Performance of the binary classification will be measured as follows:</p>
                    <ul>
                        <li>Recall = #correct_answers / #problems</li>
                        <li>Precision = #correct_answers / #answers</li>
                    </ul>
                    <p>Participants are be ranked by combining these measures via F1.</p>
                    <p>In addition, participants may also provide a score, a real number in the set [0,1] inclusive,
                        where 0 corresponds to NO and 1 to YES. A separate ranking for those participants who also
                        submit real scores [0,1] according to the ROC-AUC. For the calculation of ROC curves, any
                        missing answers are assumed to be wrong answers.</p>


                    <h3>Test Corpus</h3>
                    <p>Once you finished tuning your approach to achieve satisfying performance on the training corpus,
                        you should run your software on the test corpus.</p>
                    <p>During the competition, the test corpus will not be released publicly. Instead, we ask you to
                        submit your software for evaluation at our site as described below.</p>
                    <p>After the competition, the test corpus is available including ground truth data. This way, you
                        have all the necessities to evaluate your approach on your own, yet being comparable to those
                        who took part in the competition.</p>
                    <p>
                        <a class="uk-button uk-button-primary" target="_blank"
                           href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-13/pan13-data/pan13-authorship-verification-test-corpus1-2013-02-28.zip">Download
                            corpus 1</a>
                        <a class="uk-button uk-button-primary" target="_blank"
                           href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-13/pan13-data/pan13-authorship-verification-test-corpus2-2013-05-29.zip">Download
                            corpus 2</a>
                    </p>


                    <h3>Submission</h3>
                    <p>We ask you to prepare your software so that it can be executed via a command line call. You can
                        choose freely among the available programming languages and among the operating systems
                        Microsoft Windows 7 and Ubuntu 12.04. We will ask you to deploy your software onto a virtual
                        machine that will be made accessible to you after registration. You will be able to reach the
                        virtual machine via ssh and via remote desktop. Please test your software using one of the
                        unit-test-scripts below. Download the script, fill in the required fields, and start it using
                        the sh command. If the script runs without errors and if the correct output is produced, you can
                        submit your software by sending your unit-test-script via e-mail to <a
                                href="mailto:pan@webis.de">pan@webis.de</a>. For more information see the PAN 2013 User
                        Guide below.</p>
                    <p>
                        <a class="uk-button uk-button-primary" href="pan13-virtual-machine-user-guide.pdf">PAN User Guide »</a>
                        <a class="uk-button uk-button-primary" href="../pan13-code/unit-test-ta-windows.sh">Unit-Test Windows »</a>
                        <a class="uk-button uk-button-primary" href="../pan13-code/unit-test-ta-ubuntu.sh">Unit-Test Ubuntu »</a>
                    </p>
                    <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to grant
                        us usage rights only for the purpose of the PAN competition. We agree not to share your software
                        with a third party or use it for other purposes than the PAN competition.</p>


                    <h3>Results</h3>
                    <p>The following table lists the performances achieved by the participating teams:</p>
                    <table class="uk-table uk-table-divider uk-table-small uk-table-hover">
                        </thead>
                        <tbody>
                        <tr>
                            <td>0.753</td>
                            <td>Shachar Seidman<br/>Bar Ilan University, Israel</td>
                        </tr>
                        <tr>
                            <td>0.718</td>
                            <td>Oren Halvani, Martin Steinebach, and Ralf Zimmermann<br/>Fraunhofer Institute for Secure
                                Information Technology SIT, Germany
                            </td>
                        </tr>
                        <tr>
                            <td>0.671</td>
                            <td>Robert Layton, Paul Watters, and Richard Dazeley<br/>University of Ballarat, Australia
                            </td>
                        </tr>
                        <tr>
                            <td>0.671</td>
                            <td>Timo Petmanson<br/>University of Tartu, Estonia</td>
                        </tr>
                        <tr>
                            <td>0.659</td>
                            <td>Magdalena Jankowska, Vlado Kešelj, and Evangelos Milios<br/>Dalhousie University, Canada
                            </td>
                        </tr>
                        <tr>
                            <td>0.659</td>
                            <td>Darnes Vilariño, David Pinto, Helena Gómez, Saúl León, and Esteban Castillo<br/>Benemérita
                                Universidad Autónoma de Puebla, Mexico
                            </td>
                        </tr>
                        <tr>
                            <td>0.655</td>
                            <td>Victoria Bobicev<br/>Technical University of Moldova, Moldova</td>
                        </tr>
                        <tr>
                            <td>0.647</td>
                            <td>Vanessa Wei Feng and Graeme Hirst<br/>University of Toronto, Canada</td>
                        </tr>
                        <tr>
                            <td>0.612</td>
                            <td>Paola Ledesma°, Gibran Fuentes*, Gabriela Jasso*, Angel Toledo*, and Ivan Meza*<br/>°Escuela
                                Nacional de Antropología e Historia (ENAH) and *Universidad Nacional Autónoma de México
                                (UNAM), Mexico
                            </td>
                        </tr>
                        <tr>
                            <td>0.606</td>
                            <td>M.R. Ghaeini<br/>Amirkabir University of Technology, Iran</td>
                        </tr>
                        <tr>
                            <td>0.600</td>
                            <td>Michiel van Dam<br/>Delft University of Technology, The Netherlands</td>
                        </tr>
                        <tr>
                            <td>0.600</td>
                            <td>Erwan Moreau and Carl Vogel<br/>Trinity College Dublin, Ireland</td>
                        </tr>
                        <tr>
                            <td>0.576</td>
                            <td>Arun Jayapal and Binayak Goswami<br/>Nuance Communications, India</td>
                        </tr>
                        <tr>
                            <td>0.553</td>
                            <td>Cristian Grozea° and Marius Popescu*<br/>°Fraunhofer FIRST, Germany, and *University of
                                Bucharest, Romania
                            </td>
                        </tr>
                        <tr>
                            <td>0.541</td>
                            <td>Anna Vartapetiance and Lee Gillam<br/>University of Surrey, UK</td>
                        </tr>
                        <tr>
                            <td>0.529</td>
                            <td>Roman Kern<br/>Know-Center GmbH, Austria</td>
                        </tr>
                        <tr>
                            <td>0.500</td>
                            <td>Baseline</td>
                        </tr>
                        <tr>
                            <td>0.417</td>
                            <td>Cor J. Veenman° and Zhenshi Li*<br/>°Netherlands Forensic Institute and *Delft
                                University of Technology, The Netherlands
                            </td>
                        </tr>
                        <tr>
                            <td>0.331</td>
                            <td>Sorin Fratila<br/>University Politehnica of Bucharest, Romania</td>
                        </tr>
                        </tbody>
                    </table>
                    <p>A more detailed analysis of the detection performances can be found in the overview paper
                        accompanying this task.</p>
                    <p>
                        <a class="uk-button uk-button-primary"
                           href="https://www.uni-weimar.de/medien/webis/events/pan-13/pan13-papers-final/pan13-authorship-verification/juola13-overview.pdf#page=8">Learn
                            more »</a>
                    </p>


                    <h3>Related Work</h3>
                    <p>We refer you to:</p>
                    <ul>
                        <li>
                            <a href="../../clef12/pan12-web/about.html#proceedings">PAN @ CLEF'12</a> (<a
                                href="https://www.uni-weimar.de/medien/webis/events/pan-12/pan12-papers-final/pan12-author-identification/juola12-overview.pdf">overview
                            paper</a>),
                        </li>
                        <li>
                            <a href="../../clef11/pan11-web/about.html#proceedings">PAN @ CLEF'11</a> (<a
                                href="https://www.uni-weimar.de/medien/webis/events/pan-11/pan11-papers-final/pan11-author-identification/argamon11-overview.pdf">overview
                            paper</a>),
                        </li>
                        <li>
                            Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                            Attribution</a>. In Foundations and Trends in
                            Retrieval, Volume 1, Issue 3, December 2006.
                        </li>
                        <li>
                            Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                                href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
                            Authorship Attribution</a>. Journal of the American Society for Information Science and
                            Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                        </li>
                        <li>
                            Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                            Survey of Modern Authorship Attribution Methods</a>.
                            of the American Society for Information Science and Technology, Volume 60, Issue 3,
                            pages 538-556, March 2009.
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="uk-section uk-section-muted">
            <div class="uk-container">

                <h2>Task Chair</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/stamatatos.html %}
                </div>

                <h2>Task Committee</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/juola.html %}
                    {% include people-cards/argamon.html %}
                    {% include people-cards/koppel.html %}
                </div>
            </div>
        </div>
    </section>
</main>
